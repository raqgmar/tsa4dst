{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHP5VOu0oiCD"
   },
   "source": [
    "# 0 Preparación del entorno.\n",
    "\n",
    "## 0.1 Definición de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "xussTphZSYVz"
   },
   "outputs": [],
   "source": [
    "tfm_path='C:/Users/raqga/OneDrive - Universidad Complutense de Madrid (UCM)/Documentos/tsa4dst/TFM_data/'\n",
    "H1_code = 'OMNI2_H0_MRG1HR'\n",
    "M5_code = 'OMNI_HRO2_5MIN'\n",
    "lookback = 12\n",
    "lookforward = 6\n",
    "tfm_path_Nh_models = f'PRED_{lookforward}h/'\n",
    "cols_to_use = ['Bx', 'By_gse', 'Bz_gse', 'By_gsm', 'Bz_gsm', 'P_density', 'E_field', 'plasma_T', 'plasma_V', 'Dst'] # 'AP', out\n",
    "col_to_predict = \"Dst\"\n",
    "hstorms_data = 'historical_storms_gruet2018.csv'\n",
    "weak_threshold = -30 #1\n",
    "moderate_threshold = -50 #2\n",
    "strong_threshold = -100 #3\n",
    "severe_threshold = -200 #4\n",
    "great_threshold = -300 #5\n",
    "gamma_value=0.0001\n",
    "temporal_margin=5*24 # margen para obtener tiempos ampliados de las tormentas de gruet et al 2018\n",
    "test_size = 0.2\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTnEx3feo7e4"
   },
   "source": [
    "## 0.2 Montar Google Drive (obtención de datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "n2tXbl93Tndp"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !rm -rf sample_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNzRT2z-o-ps"
   },
   "source": [
    "## 0.3 Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32213,
     "status": "ok",
     "timestamp": 1719831629912,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "OJtQv_kvy3ky",
    "outputId": "23d7858f-5abc-4373-fdb2-42842871bbe0"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 3792,
     "status": "ok",
     "timestamp": 1719831633701,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "sJuDqU5SzL4m"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# librerías de manipulación de datos y gráficos\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "\n",
    "# gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# modelo\n",
    "from sklearn.svm import SVR\n",
    "#from thundersvm import SVR\n",
    "# escalado y división en train/test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# obtención de métricas\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_log_error, median_absolute_error\n",
    "from sklearn.metrics import explained_variance_score, max_error\n",
    "\n",
    "# meta\n",
    "# timer\n",
    "import time\n",
    "\n",
    "#optuna\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1719831633702,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "5zz1d21Fp6sn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cUAiVDJpFBD"
   },
   "source": [
    "## 0.4 Definición de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1719831633702,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "MKGLdDwbtVg3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def exploracion_inicial_datos(df):\n",
    "    \"\"\"\n",
    "    Función para realizar una exploración inicial de los datos.\n",
    "\n",
    "    Parámetros:\n",
    "    df (dataframe): El dataframe que contiene los datos.\n",
    "\n",
    "    Muestra las primeras filas, estadísticas descriptivas, valores faltantes,\n",
    "    histogramas de variables numéricas y un mapa de calor de la correlación.\n",
    "    \"\"\"\n",
    "    # Configuración de visualización\n",
    "    sns.set(style=\"whitegrid\")  # Estilo de gráficos\n",
    "\n",
    "\n",
    "    print(\"Primeras filas del DataFrame:\")\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "    print(\"\\nDescripción estadística de los datos:\")\n",
    "    print(df.describe())\n",
    "\n",
    "\n",
    "    print(\"\\nValores faltantes por columna:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "\n",
    "    print(\"\\nVisualización de histogramas para variables numéricas:\")\n",
    "    df.hist(bins=15, figsize=(15, 10), layout=(5, 4))\n",
    "    plt.show()\n",
    "    print(\"\\nMapa de calor de la matriz de correlación:\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calcular_layout_optimo(num_cols):\n",
    "    \"\"\"\n",
    "    Calcula el número óptimo de filas y columnas para una figura con subgráficos,\n",
    "    tratando de mantener una forma que sea visualmente agradable y que aproveche el espacio.\n",
    "\n",
    "    Parámetros:\n",
    "    num_cols (int): Número total de columnas (gráficos) a mostrar.\n",
    "\n",
    "    Retorna:\n",
    "    (int, int): Número de filas y columnas para el layout de los subgráficos.\n",
    "    \"\"\"\n",
    "    # Calcula el número óptimo de columnas teniendo un límite visual razonable\n",
    "    cols_per_row = int(np.sqrt(num_cols)) + 1  # Ajuste para maximizar el uso del espacio y la forma de la figura\n",
    "    rows_needed = (num_cols + cols_per_row - 1) // cols_per_row  # Redondeo hacia arriba para incluir todas las columnas\n",
    "    return rows_needed, cols_per_row\n",
    "\n",
    "def exploracion_histogramas(df):\n",
    "    \"\"\"\n",
    "    Función para generar histogramas para todas las columnas numéricas en un DataFrame,\n",
    "    excluyendo las columnas de tipo datetime y no numéricas.\n",
    "\n",
    "    Parámetros:\n",
    "    df (DataFrame): DataFrame de pandas con los datos a analizar.\n",
    "    \"\"\"\n",
    "    # Eliminar columnas no numéricas y de tipo datetime\n",
    "    df_numerico = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Número de columnas numéricas\n",
    "    num_cols = df_numerico.shape[1]\n",
    "\n",
    "    # Verificar si hay columnas para mostrar\n",
    "    if num_cols == 0:\n",
    "        print(\"No hay columnas numéricas para mostrar.\")\n",
    "        return\n",
    "\n",
    "    # Calculando el layout necesario\n",
    "    rows_needed, cols_per_row = calcular_layout_optimo(num_cols)\n",
    "\n",
    "    # Crear histogramas\n",
    "    df_numerico.hist(bins=15, figsize=(15, 10), layout=(rows_needed, cols_per_row))\n",
    "    plt.show()\n",
    "\n",
    "def imputar_nan(df):\n",
    "  df.interpolate(method='linear', inplace=True)\n",
    "  df.fillna(method='ffill', inplace=True)\n",
    "  df.fillna(method='bfill', inplace=True)\n",
    "  if sum(df.isnull().sum())!=0:\n",
    "    print(\"Faltan nulos por tratar\")\n",
    "  return df\n",
    "\n",
    "def visualizar_nulos_plot(df, variable_with_nans):\n",
    "    \"\"\"\n",
    "    Plot the specified 'variable_with_nans' column and 'Dst' column from the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame containing the data to plot.\n",
    "    - variable_with_nans: str, the name of the column in the DataFrame to plot, which may contain NaNs.\n",
    "\n",
    "    The function assumes that 'Dst' is a column name in the DataFrame and that the DataFrame's index is suitable for plotting (e.g., datetime).\n",
    "    \"\"\"\n",
    "    # Create the figure and subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20, 8))\n",
    "\n",
    "    # Handling NaNs in the 'variable_with_nans' column before plotting\n",
    "    df_plot = df.copy()\n",
    "    df_plot[variable_with_nans] = df_plot[variable_with_nans].fillna(method='ffill')  # Forward fill to handle NaNs\n",
    "\n",
    "    # Plotting 'variable_with_nans' on the first subplot\n",
    "    ax1.scatter(df_plot.index, df_plot[variable_with_nans], label=variable_with_nans, color='blue')\n",
    "    ax1.set_ylabel(variable_with_nans)\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plotting 'Dst' on the second subplot\n",
    "    ax2.plot(df_plot.index, df_plot['Dst'], label='Dst', color='red')\n",
    "    ax2.set_ylabel('Dst')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Setting the x-axis label only on the bottom subplot\n",
    "    ax2.set_xlabel('Datetime')\n",
    "\n",
    "    # Improve layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example of how to use this function with a DataFrame containing NaNs\n",
    "data = {\n",
    "    'Datetime': pd.date_range(start='2021-01-01', periods=100, freq='D'),\n",
    "    'variable_with_nans': pd.Series(range(100)).where(lambda x : x % 10 != 0),\n",
    "    'Dst': range(100, 0, -1)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Datetime', inplace=True)\n",
    "\n",
    "def create_window_df_svr(list_dfs, lookback, lookforward, cols_to_use, col_to_predict, scaler_label=None):\n",
    "    \"\"\"\n",
    "    Creates input and output datasets for SVR training from a list of DataFrames, incorporating windowing and optional descaling for target\n",
    "\n",
    "    Parameters:\n",
    "    list_dfs (list of pandas.DataFrame): List of DataFrames to process.\n",
    "    lookback (int): Number of past records to include as features for each prediction.\n",
    "    lookforward (int): Number of records ahead to predict.\n",
    "    cols_to_use (list of str): List of column names to use as features.\n",
    "    col_to_predict (str): Column name to predict.\n",
    "    scaler_label (StandardScaler, optional): Scaler for the output variable, used for inverse transformation.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing arrays for training features and labels.\n",
    "    \"\"\"\n",
    "    x_train, y_train = [], []\n",
    "\n",
    "    for df_ in list_dfs:\n",
    "        df = df_.copy()\n",
    "\n",
    "        for i in range(len(df) - lookback - lookforward + 1):\n",
    "            x_train.append(np.asarray(df.iloc[i:i+lookback][cols_to_use].values))\n",
    "            y_train.append(np.asarray(df.iloc[i+lookback][col_to_predict]))\n",
    "\n",
    "    if scaler_label is not None:\n",
    "        y_train = scaler_label.inverse_transform(np.asarray(y_train).reshape(-1,1))\n",
    "\n",
    "    return np.asarray(x_train), np.asarray(y_train)\n",
    "\n",
    "\n",
    "def filter_storms(df, historical_storms, temporal_margin):\n",
    "    \"\"\"\n",
    "    Filter DataFrame entries based on the occurrence of storms within specific time intervals.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing time-series data with a 'Datetime' column.\n",
    "    historical_storms (pandas.DataFrame): DataFrame containing the start and end times of historical storms.\n",
    "    temporal_margin (int): Number of rows before and after the minimum Dst index to include in the result.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of DataFrame snippets corresponding to the specified storm intervals.\n",
    "    \"\"\"\n",
    "    all_storms = []\n",
    "    for i in range(len(historical_storms)):\n",
    "        df_tmp = df[(df[\"Datetime\"] >= historical_storms.iloc[i][\"start\"]) & (df[\"Datetime\"] <= historical_storms.iloc[i][\"end\"])]\n",
    "        idx = df_tmp['Dst'].idxmin()\n",
    "        all_storms.append(df.iloc[idx-temporal_margin:idx+temporal_margin])\n",
    "    return all_storms\n",
    "\n",
    "def combinar_dataframes_solapados(dfs):\n",
    "    \"\"\"\n",
    "    Combines overlapping DataFrames in a list into non-overlapping DataFrames based on the 'Datetime' column.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list of pandas.DataFrame): List of DataFrames to combine.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of combined DataFrames without overlap.\n",
    "    \"\"\"\n",
    "    dfs.sort(key=lambda x: x['Datetime'].min())\n",
    "    combinados = []\n",
    "    combinacion_actual = dfs[0]\n",
    "\n",
    "    for df in dfs[1:]:\n",
    "        if df['Datetime'].min() <= combinacion_actual['Datetime'].max():\n",
    "            combinacion_actual = pd.concat([combinacion_actual, df]).drop_duplicates().sort_values(by='Datetime')\n",
    "        else:\n",
    "            combinados.append(combinacion_actual)\n",
    "            combinacion_actual = df\n",
    "    combinados.append(combinacion_actual)\n",
    "    return combinados\n",
    "\n",
    "def scale_data(list_dfs, cols_to_use, col_to_predict):\n",
    "    \"\"\"\n",
    "    Scales columns in a list of DataFrames using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    list_dfs (list of pandas.DataFrame): List of DataFrames to scale.\n",
    "    cols_to_use (list of str): Column names to apply scaling to.\n",
    "    col_to_predict (str): Column name used as a label for prediction.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the list of scaled DataFrames and the label scaler.\n",
    "    \"\"\"\n",
    "    list_dfs_ = []\n",
    "    scaler_cols = StandardScaler()\n",
    "    scaler_label = StandardScaler()\n",
    "    scaler_cols.fit(pd.concat(list_dfs)[cols_to_use])\n",
    "    scaler_label.fit(np.asarray(pd.concat(list_dfs)[col_to_predict]).reshape(-1,1))\n",
    "\n",
    "    for df_ in list_dfs:\n",
    "        df = df_.copy()\n",
    "        df[cols_to_use] = scaler_cols.transform(df[cols_to_use])\n",
    "        list_dfs_.append(df)\n",
    "\n",
    "    return list_dfs_, scaler_label\n",
    "\n",
    "\n",
    "\n",
    "def calc_metrics(predictions, y_test):\n",
    "  # Mean Squared Error\n",
    "  mse = mean_squared_error(y_test, predictions)\n",
    "  # Mean Absolute Error\n",
    "  mae = mean_absolute_error(y_test, predictions)\n",
    "  # R^2 Score, the coefficient of determination\n",
    "  r2 = r2_score(y_test, predictions)\n",
    "  # Median Absolute Error\n",
    "  medae = median_absolute_error(y_test, predictions)\n",
    "  # Explained Variance Score\n",
    "  explained_variance = explained_variance_score(y_test, predictions)\n",
    "  # Max Error\n",
    "  max_err = max_error(y_test, predictions)\n",
    "\n",
    "  return mse, mae, r2, medae, explained_variance, max_err\n",
    "\n",
    "\n",
    "def formatear_tiempo(segundos):\n",
    "    horas = int(segundos // 3600)\n",
    "    minutos = int((segundos % 3600) // 60)\n",
    "    segundos = segundos % 60\n",
    "    return f\"{horas} horas, {minutos} minutos, {segundos:.2f} segundos\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719831633702,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "dhsyk8pnCibA"
   },
   "outputs": [],
   "source": [
    "def evaluate_svr_models(X_train, y_train, X_test, y_test, kernels, C_values, epsilon_values, gamma_values, degree_values, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate SVR models with different hyperparameter configurations and kernel types.\n",
    "\n",
    "    Args:\n",
    "    X_train (array): Independent training data.\n",
    "    y_train (array): Dependent training data (target).\n",
    "    X_test (array): Independent test data.\n",
    "    y_test (array): Dependent test data (target).\n",
    "    kernels (list): List of kernel types to evaluate.\n",
    "    C_values (list): List of values for the penalty parameter C.\n",
    "    epsilon_values (list): List of values for the epsilon parameter.\n",
    "    gamma_values (list): List of values for the gamma parameter.\n",
    "    degree_values (list): List of values for the degree parameter (used only in polynomial kernels).\n",
    "    verbose (bool, optional): If True, prints messages during the evaluation process. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing evaluation metrics for each parameter configuration.\n",
    "    dict: A dictionary of trained models, with keys describing the specific parameter configuration.\n",
    "\n",
    "    Note:\n",
    "    Assumes that the 'precomputed' kernel is only used if the 'precomputed_matrix' is defined in the local environment.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    models_dict = {}\n",
    "\n",
    "    for kernel_ in kernels:\n",
    "        for C in C_values:\n",
    "            for epsilon in epsilon_values:\n",
    "                # Check parameter relevance for gamma and degree for the current kernel\n",
    "                relevant_gamma_values = gamma_values if kernel_ in ['rbf', 'sigmoid', 'poly'] else [None]\n",
    "                relevant_degree_values = degree_values if kernel_ == 'poly' else [None]\n",
    "\n",
    "                for gamma in relevant_gamma_values:\n",
    "                    for degree in relevant_degree_values:\n",
    "                        start = time.time()\n",
    "                        config_key = f\"{kernel_}_C{C}_eps{epsilon}_gamma{gamma}_deg{degree}\"\n",
    "                        if verbose:\n",
    "                            print(f\"Starting {config_key}\")\n",
    "\n",
    "                        if kernel_ == \"precomputed\":\n",
    "                            if 'precomputed_matrix' in locals():\n",
    "                                model = SVR(kernel='precomputed', C=C, epsilon=epsilon)\n",
    "                                model.fit(precomputed_matrix, y_train)\n",
    "                                models_dict[config_key] = model\n",
    "                                continue\n",
    "                            else:\n",
    "                                if verbose:\n",
    "                                    print(\"Precomputed matrix not defined for kernel='precomputed'\")\n",
    "                                continue\n",
    "\n",
    "                        model = SVR(kernel=kernel_, C=C, epsilon=epsilon, gamma=(gamma if gamma is not None else 0.01), degree=(degree if degree is not None else 3))\n",
    "                        model.fit(X_train, y_train)\n",
    "                        models_dict[config_key] = model\n",
    "\n",
    "                        predictions = model.predict(X_test)\n",
    "                        mse, mae, r2, medae, explained_variance, max_err = calc_metrics(predictions, y_test)\n",
    "                        end = time.time()\n",
    "                        time_exec = formatear_tiempo(end - start)\n",
    "\n",
    "                        results.append({\n",
    "                            \"kernel\": kernel_,\n",
    "                            \"C\": C,\n",
    "                            \"epsilon\": epsilon,\n",
    "                            \"gamma\": gamma,\n",
    "                            \"degree\": degree,\n",
    "                            \"mse\": mse,\n",
    "                            \"mae\": mae,\n",
    "                            \"r2\": r2,\n",
    "                            \"medae\": medae,\n",
    "                            \"exp_var\": explained_variance,\n",
    "                            \"max_err\": max_err,\n",
    "                            \"time_exec\": time_exec\n",
    "                        })\n",
    "\n",
    "                        print(f\"{config_key} finished. Time for iteration: {time_exec} | mae: {mae} | mse: {mse} | exp_var: {explained_variance} | max_err: {max_err}\")\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results, models_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiZeUiNVpNrd"
   },
   "source": [
    "# 1. Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt5CN8t9pnMA"
   },
   "source": [
    "## 1.1 Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 1257,
     "status": "ok",
     "timestamp": 1719831634956,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "o0zK7ahApmQf"
   },
   "outputs": [],
   "source": [
    "hd = pd.read_csv(tfm_path+H1_code+'.csv', parse_dates=[\"Datetime\"])\n",
    "# md = pd.read_csv(tfm_path+M5_code+'.csv', parse_dates=[\"Datetime\"]) # no se va a usar por ahora\n",
    "historical_storms = pd.read_csv(tfm_path+hstorms_data)\n",
    "# historical_storms = historical_storms.drop(columns=['Min. Dst (nT)','Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4mHmDLrtEKs"
   },
   "source": [
    "- Ver qué columnas y tipo de datos contienen los df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "up24FqxwscRS",
    "outputId": "536f508f-4b13-4eb8-b18f-af4d7001a4de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID_IMF', 'ID_plasma', 'Bmag', 'dev_Bmag', 'Bx', 'By_gse', 'Bz_gse',\n",
       "       'By_gsm', 'Bz_gsm', 'dev_Bx', 'dev_By', 'dev_Bz', 'P_density',\n",
       "       'dev_P_density', 'AP', 'dev_AP', 'E_field', 'plasma_T', 'dev_plasma_T',\n",
       "       'plasma_V', 'Dst', 'Datetime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "f3lGenT5tId9",
    "outputId": "0e19c4e4-17da-4544-b3ed-7ffb13dd3881"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_IMF                  float64\n",
       "ID_plasma               float64\n",
       "Bmag                    float64\n",
       "dev_Bmag                float64\n",
       "Bx                      float64\n",
       "By_gse                  float64\n",
       "Bz_gse                  float64\n",
       "By_gsm                  float64\n",
       "Bz_gsm                  float64\n",
       "dev_Bx                  float64\n",
       "dev_By                  float64\n",
       "dev_Bz                  float64\n",
       "P_density               float64\n",
       "dev_P_density           float64\n",
       "AP                      float64\n",
       "dev_AP                  float64\n",
       "E_field                 float64\n",
       "plasma_T                float64\n",
       "dev_plasma_T            float64\n",
       "plasma_V                float64\n",
       "Dst                     float64\n",
       "Datetime         datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "2s-_PYArsgz4",
    "outputId": "69ea9159-795b-4dc6-dc10-fd17097b45ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Min. Dst (nT)', 'start', 'end', 'storm'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_storms.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "oq6T4hJDsm3Q",
    "outputId": "53c5b60c-976a-499b-c55c-560ba4585a11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        int64\n",
       "Min. Dst (nT)     int64\n",
       "start            object\n",
       "end              object\n",
       "storm             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_storms.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyL09t1Ks9jW"
   },
   "source": [
    "- Ordenar las tormentas en caso de que no lo estén\n",
    "- Convertir todas las fechas a datetime de pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "KKh0YUUIq6xc"
   },
   "outputs": [],
   "source": [
    "# Ordenar los dataframes por fecha\n",
    "historical_storms = historical_storms.sort_values(by='start')\n",
    "\n",
    "# convertir las columnas de tiempo a datetime64\n",
    "hd['Datetime']=pd.to_datetime(hd['Datetime'])\n",
    "historical_storms['start']=pd.to_datetime(historical_storms['start'])\n",
    "historical_storms['end']=pd.to_datetime(historical_storms['end'])\n",
    "\n",
    "# Cuando se utilizan los datos a 5 minutos, se unen a 5min\n",
    "#data = pd.merge(md, hd[[\"Datetime\", \"Dst\"]], on='Datetime', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzCZp1Ndpt_F"
   },
   "source": [
    "## 1.2 Exploración inicial de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sngrxbi1uXRA"
   },
   "source": [
    "- head del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "lavp1YMbuOQZ"
   },
   "outputs": [],
   "source": [
    "# hd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVAqYfC9uU3P"
   },
   "source": [
    "- descripción estadística de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "t0-OstdCuSlD"
   },
   "outputs": [],
   "source": [
    "# hd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9Ry8TktulyE"
   },
   "source": [
    "- ver los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "xnw_H2T-ulg5"
   },
   "outputs": [],
   "source": [
    "# hd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "d-LN6q6fuixi"
   },
   "outputs": [],
   "source": [
    "# hd.isnull().sum() / len(hd) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx6zks84uzv8"
   },
   "source": [
    "- Histograma para variables numéricas (todas, en este caso, pero hemos quitado datetime porque no es muy util al ser una serie temporal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "ke7ogtCzu0KB"
   },
   "outputs": [],
   "source": [
    "# exploracion_histogramas(hd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkDpVZ15xPMM"
   },
   "source": [
    "- Visualizaciónd e historical_storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719831634958,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "Vr2COnTg0D6S"
   },
   "outputs": [],
   "source": [
    "# storms_data = historical_storms.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831634958,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "4ivSRmTdz-Kf"
   },
   "outputs": [],
   "source": [
    "# # Convertir las columnas de fecha y hora a datetime\n",
    "# storms_data['start'] = pd.to_datetime(storms_data['start'])\n",
    "# storms_data['end'] = pd.to_datetime(storms_data['end'])\n",
    "# # Calcular la duración de cada tormenta en horas\n",
    "# storms_data['duration_hours'] = (storms_data['end'] - storms_data['start']).dt.total_seconds() / 3600\n",
    "\n",
    "# # Gráfico de la distribución de las intensidades mínimas de las tormentas\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# sns.histplot(storms_data['Min. Dst (nT)'], bins=30, kde=True, color='blue')\n",
    "# plt.title('Distribución de Intensidades Mínimas de Tormentas Geomagnéticas')\n",
    "# plt.xlabel('Min. Dst (nT)')\n",
    "# plt.ylabel('Frecuencia')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnkxO8Zk0pnw"
   },
   "source": [
    "Gráfico de dispersión entre la duración y la intensidad mínima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831634958,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "hCSpRC9N0i8E"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.scatterplot(x='duration_hours', y='Min. Dst (nT)', data=storms_data)\n",
    "# plt.title('Correlación entre Duración e Intensidad Mínima de Tormentas')\n",
    "# plt.xlabel('Duración (Horas)')\n",
    "# plt.ylabel('Min. Dst (nT)')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2tC7isT0yJI"
   },
   "source": [
    "- Ver el timeline de las tormentas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831634958,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "KfIV8r9Q00gx"
   },
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# # Crear la figura\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # Añadir cada tormenta como un segmento de línea en el gráfico\n",
    "# for i, row in storms_data.iterrows():\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=[row['start'], row['end']],\n",
    "#         y=[i, i],\n",
    "#         mode='lines+markers',\n",
    "#         name=f\"Tormenta {row['storm']}\"\n",
    "#     ))\n",
    "\n",
    "# # Ajustar la presentación de la figura\n",
    "# fig.update_layout(\n",
    "#     title=\"Línea de Tiempo de Tormentas Históricas\",\n",
    "#     xaxis_title=\"Fecha\",\n",
    "#     yaxis_title=\"Tormenta\",\n",
    "#     yaxis=dict(\n",
    "#         tickmode='array',\n",
    "#         tickvals=list(range(len(storms_data)))\n",
    "#     ),\n",
    "#     showlegend=False,\n",
    "#     xaxis=dict(\n",
    "#         rangeslider=dict(\n",
    "#             visible=True\n",
    "#         ),\n",
    "#         type=\"date\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Mostrar la figura\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ABxa-pMvUiR"
   },
   "source": [
    "Mapa de calor de la matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831634958,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "ttbLXb7UvWm0"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 8))\n",
    "# sns.heatmap(hd.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-5Cz91Ap3RH"
   },
   "source": [
    "## 1.3 Limpieza y procesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4RKW9php63d"
   },
   "source": [
    "### 1.3.1 Tratamiento de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1719831635295,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "t7usCL6U8a6U",
    "outputId": "88e0a147-1ef4-402f-815d-fa0902645794"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42472"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(hd.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719831635295,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "FBM6gfRL85Tb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raqga\\AppData\\Local\\Temp\\ipykernel_7184\\2672241348.py:80: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\raqga\\AppData\\Local\\Temp\\ipykernel_7184\\2672241348.py:81: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "hd = imputar_nan(hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831635295,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "AwfFb2yZ8-dU",
    "outputId": "ff1eb87e-72e3-4471-f1ce-66cf42dfb9de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(hd.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn3SwS3Np93N"
   },
   "source": [
    "### 1.3.2 Normalización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1719831635719,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "tSl0lkmfrHEz"
   },
   "outputs": [],
   "source": [
    "all_storms = filter_storms(hd, historical_storms, temporal_margin)\n",
    "all_storms = combinar_dataframes_solapados(all_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1719831636440,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "z9Fj84tVSCwr"
   },
   "outputs": [],
   "source": [
    "pd.concat(all_storms).to_csv(tfm_path+\"all_storms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719831636441,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "duerUHXhIFa5"
   },
   "outputs": [],
   "source": [
    "storms_check = pd.concat(all_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1719831636910,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "dWxN6o_sIFsf"
   },
   "outputs": [],
   "source": [
    "all_storms, scaler_target = scale_data(all_storms, cols_to_use, col_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSZtUWlLqCAU"
   },
   "source": [
    "### 1.3.3 Codificación de variables categóricas\n",
    "\n",
    "* No existe codificación de variables categóricas porque **no hay variables categóricas). A continuación, se muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719831636910,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "Xwu8MOV3qb2D",
    "outputId": "1c915010-c71d-4b57-fd1b-087a0e97668e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_IMF                  float64\n",
       "ID_plasma               float64\n",
       "Bmag                    float64\n",
       "dev_Bmag                float64\n",
       "Bx                      float64\n",
       "By_gse                  float64\n",
       "Bz_gse                  float64\n",
       "By_gsm                  float64\n",
       "Bz_gsm                  float64\n",
       "dev_Bx                  float64\n",
       "dev_By                  float64\n",
       "dev_Bz                  float64\n",
       "P_density               float64\n",
       "dev_P_density           float64\n",
       "AP                      float64\n",
       "dev_AP                  float64\n",
       "E_field                 float64\n",
       "plasma_T                float64\n",
       "dev_plasma_T            float64\n",
       "plasma_V                float64\n",
       "Dst                     float64\n",
       "Datetime         datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(all_storms).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULHHGwCjqr6C"
   },
   "source": [
    "# 3 Entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zqWtTvA4Ycl"
   },
   "source": [
    "## 3.1 Creación de la ventana temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 19565,
     "status": "ok",
     "timestamp": 1719831656473,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "XAm8sTsD4WrQ"
   },
   "outputs": [],
   "source": [
    "# x_window, y_window = create_window_df_svr(all_storms, lookforward, cols_to_use, col_to_predict, scaler_target) # desescalado\n",
    "x_window, y_window = create_window_df_svr(all_storms, lookback, lookforward, cols_to_use, col_to_predict) # sin desescalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWNESFbg5L_C"
   },
   "source": [
    "## 3.2 División en train-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1719831656474,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "GIqtYB085ONF"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_window, y_window, test_size=test_size, random_state=42)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)  # Transforma a (n_samples, n_features*lookback)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)  # Lo mismo para el conjunto de prueba\n",
    "#X_train = np.squeeze(X_train, axis=1)\n",
    "#X_test = np.squeeze(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2EUAcFYM2ez"
   },
   "source": [
    "## 4 Optimización de la selección de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719831656474,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "9xV7WmtdM5p2"
   },
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     # Suggesting hyperparameters using suggest_float\n",
    "#     C = trial.suggest_float('C', 1e-6, 1e2, log=True)  # Logarithmic for C\n",
    "#     gamma = trial.suggest_float('gamma', 1e-6, 1e2, log=True)  # Logarithmic for gamma\n",
    "#     epsilon = trial.suggest_float('epsilon', 0.001, 0.1)  # Linear for epsilon\n",
    "#     degree = trial.suggest_int('degree', 1, 10)  # Linear for epsilon\n",
    "#     kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n",
    "\n",
    "#     # Create and train the SVR model from Thunder SVM\n",
    "#     model = SVR(C=C, gamma=gamma, epsilon=epsilon, degree=degree, kernel=kernel)\n",
    "\n",
    "#     # Evaluate the model using cross-validation\n",
    "#     score = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error').mean()\n",
    "#     return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the full path to the SQLite database\n",
    "# database_path = os.path.join(tfm_path+tfm_path_Nh_models, 'random_20240712-kaisa-non-linear_study.db')\n",
    "# storage_url = f'sqlite:///{database_path}'\n",
    "# new_study_name=\"random_kaisa-non-linear-study-20240712\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the study with the GridSampler\n",
    "# study = optuna.create_study(study_name=new_study_name,storage=storage_url, load_if_exists=True,sampler=optuna.samplers.RandomSampler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Ejecutar la optimización\n",
    "# study.optimize(objective, n_trials=100, timeout=600, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1719831656474,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "DWN8j9R9B6AE"
   },
   "outputs": [],
   "source": [
    "# # Define the full path to the SQLite database\n",
    "# database_path = os.path.join(tfm_path+tfm_path_Nh_models, 'GRID_20240712-kaisa-linear_study.db')\n",
    "# storage_url = f'sqlite:///{database_path}'\n",
    "# new_study_name=\"GRID_kaisa-linear-study-20240712\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     C = trial.suggest_float('C', 1e-6, 1e2, log=True)\n",
    "#     gamma = trial.suggest_float('gamma', 1e-6, 1e2, log=True)\n",
    "#     epsilon = trial.suggest_float('epsilon', 0.001, 0.1)\n",
    "#     degree = trial.suggest_int('degree', 1, 10)\n",
    "#     kernel = trial.suggest_categorical('kernel', ['linear'])\n",
    "\n",
    "#     # Create and train the model, evaluate using cross-validation\n",
    "#     model = SVR(C=C, gamma=gamma, epsilon=epsilon, degree=degree, kernel=kernel)\n",
    "#     score = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error').mean()\n",
    "#     return -score\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'C': np.logspace(-6, 2, num=6),  # Fewer points for demonstration\n",
    "#     'gamma': [1], # not used in linear\n",
    "#     'epsilon': np.linspace(0.001, 0.1, num=6),\n",
    "#     'degree': [1],  # Fixed for simplification, not used in linear\n",
    "#     'kernel': ['linear']  # Fixed to 'linear'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2842,
     "status": "ok",
     "timestamp": 1719831659312,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "qJbacq4uCMRH",
    "outputId": "adcf4538-767f-44a7-8a52-c7437c082d36"
   },
   "outputs": [],
   "source": [
    "# # Create the study with the GridSampler\n",
    "# study = optuna.create_study(study_name=new_study_name,storage=storage_url, load_if_exists=True,sampler=optuna.samplers.GridSampler(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "w1RcxiJOCXfb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# study.optimize(objective, timeout=600, n_trials=len(param_grid['C']) * len(param_grid['gamma']) * len(param_grid['epsilon']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid for different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full path to the SQLite database\n",
    "database_path = os.path.join(tfm_path+tfm_path_Nh_models, f'DEF_kaisa_grid_allKernels_pred{lookforward}H_20240713.db')\n",
    "storage_url = f'sqlite:///{database_path}'\n",
    "new_study_name=f\"DEF_kaisa_grid_allKernels_pred{lookforward}_20240713\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': np.logspace(-6, 2, num=6).tolist(),\n",
    "    'gamma': np.logspace(-6, 2, num=6).tolist(),\n",
    "    'epsilon': np.linspace(0.001, 0.1, num=6).tolist(),\n",
    "    'degree': list(range(1, 11)),\n",
    "    'kernel': ['linear', 'poly', 'rbf']\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    C = trial.suggest_float('C', 1e-6, 1e2, log=True)\n",
    "    epsilon = trial.suggest_float('epsilon', 0.001, 0.1)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n",
    "\n",
    "    if kernel == 'linear':\n",
    "        gamma = 1\n",
    "        degree = 1\n",
    "    elif kernel == 'poly':\n",
    "        gamma = 1\n",
    "        degree = trial.suggest_int('degree', 1, 10)\n",
    "    elif kernel == 'rbf':\n",
    "        gamma = trial.suggest_float('gamma', 1e-6, 1e2, log=True)\n",
    "        degree = 1\n",
    "\n",
    "    model = SVR(C=C, gamma=gamma, epsilon=epsilon, degree=degree, kernel=kernel, max_iter=10000000)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error').mean()\n",
    "    return -score\n",
    "\n",
    "def print_metrics(study, trial):\n",
    "    print(f'Trial {trial.number}: Value={trial.value}, Params={trial.params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-15 17:07:21,265] Using an existing study with name 'DEF_kaisa_grid_allKernels_pred6_20240713' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=new_study_name, storage=storage_url, load_if_exists=True, sampler=optuna.samplers.GridSampler(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS=len(param_grid['C']) * len(param_grid['epsilon']) * len(param_grid['kernel'])\n",
    "print(NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# study.optimize(objective, n_trials=NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ-V-0cQqtmf"
   },
   "source": [
    "# 4 Evaluación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "EP3W14kOjPpK"
   },
   "outputs": [],
   "source": [
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "P1PUpDA-j4lJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.5118864315095824, 'epsilon': 0.08020000000000001, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "3A6BnhkAkyQH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_C2.5118864315095824_eps0.08020000000000001_gammaNone_degNone finished. Time for iteration: 0 horas, 6 minutos, 9.13 segundos | mae: 0.09061414290776948 | mse: 0.021349473697620334 | exp_var: 0.9808556856995324 | max_err: 1.3075196401258449\n"
     ]
    }
   ],
   "source": [
    "predictions, best_model_dict = evaluate_svr_models(X_train, y_train, X_test, y_test, [best_params['kernel']], [best_params['C']], [best_params['epsilon']], 1, 1, verbose=False)\n",
    "# , [best_params['gamma']], [best_params['degree']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "1_8zaUoEkzQS"
   },
   "outputs": [],
   "source": [
    "best_model = best_model_dict[next(iter(best_model_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "hB5bqIqhlF9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | rmse: 5.575357737743236 | mse: 31.084613903813374| mae: 3.327043923395395 | r2: 0.9779416153344285 | medae: 2.159803528128325 | explained_variance: 0.9779468190865999 | max_err: 101.3994111258386\n",
      "Test | rmse: 5.428690722501532 | mse: 29.47068296057421| mae: 3.3666463672607057 | r2: 0.9808513296090985 | medae: 2.2591486656156974 | explained_variance: 0.9808556856995324 | max_err: 48.57913020302118\n"
     ]
    }
   ],
   "source": [
    "# Assume y_pred are the predictions from the model\n",
    "y_pred_scaled = best_model.predict(X_train)\n",
    "y_pred = scaler_target.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_train = scaler_target.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Assume y_pred are the predictions from the model\n",
    "y_pred_scaledT = best_model.predict(X_test)\n",
    "y_pred_t = scaler_target.inverse_transform(y_pred_scaledT.reshape(-1, 1)).flatten()\n",
    "y_test = scaler_target.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "mse, mae, r2, medae, exp_var, max_err = calc_metrics(y_pred, y_train)\n",
    "print(f\"Train | rmse: {np.sqrt(mse)} | mse: {mse}| mae: {mae} | r2: {r2} | medae: {medae} | explained_variance: {exp_var} | max_err: {max_err}\")\n",
    "mse, mae, r2, medae, exp_var, max_err = calc_metrics(y_pred_t, y_test)\n",
    "print(f\"Test | rmse: {np.sqrt(mse)} | mse: {mse}| mae: {mae} | r2: {r2} | medae: {medae} | explained_variance: {exp_var} | max_err: {max_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

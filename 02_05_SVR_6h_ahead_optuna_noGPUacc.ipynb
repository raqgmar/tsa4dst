{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqgmar/tsa4dst/blob/main/02_05_SVR_6h_ahead_optuna_noGPUacc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 Preparación del entorno.\n",
        "\n",
        "## 0.1 Definición de parámetros"
      ],
      "metadata": {
        "id": "xHP5VOu0oiCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xussTphZSYVz"
      },
      "outputs": [],
      "source": [
        "tfm_path='/content/drive/MyDrive/TFM data/'\n",
        "H1_code = 'OMNI2_H0_MRG1HR'\n",
        "M5_code = 'OMNI_HRO2_5MIN'\n",
        "lookback = 12\n",
        "lookforward = 6\n",
        "cols_to_use = ['Bx', 'By_gse', 'Bz_gse', 'By_gsm', 'Bz_gsm', 'P_density', 'E_field', 'plasma_T', 'plasma_V', 'Dst'] # 'AP', out\n",
        "col_to_predict = \"Dst\"\n",
        "hstorms_data = 'historical_storms_gruet2018.csv'\n",
        "weak_threshold = -30 #1\n",
        "moderate_threshold = -50 #2\n",
        "strong_threshold = -100 #3\n",
        "severe_threshold = -200 #4\n",
        "great_threshold = -300 #5\n",
        "gamma_value=0.0001\n",
        "temporal_margin=5*24 # margen para obtener tiempos ampliados de las tormentas de gruet et al 2018\n",
        "test_size = 0.2\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 Montar Google Drive (obtención de datos)"
      ],
      "metadata": {
        "id": "JTnEx3feo7e4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2tXbl93Tndp",
        "outputId": "71cbbcf1-0012-41c0-f679-38f37a5b0532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!rm -rf sample_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 Importación de librerías"
      ],
      "metadata": {
        "id": "WNzRT2z-o-ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# librerías de manipulación de datos y gráficos\n",
        "import pandas as pd\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "\n",
        "# gráficos\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# modelo\n",
        "from sklearn.svm import SVR\n",
        "#from thundersvm import SVR\n",
        "# escalado y división en train/test\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# obtención de métricas\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import mean_squared_log_error, median_absolute_error\n",
        "from sklearn.metrics import explained_variance_score, max_error\n",
        "\n",
        "# meta\n",
        "# timer\n",
        "import time\n",
        "\n",
        "#optuna\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "sJuDqU5SzL4m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.4 Definición de funciones"
      ],
      "metadata": {
        "id": "9cUAiVDJpFBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exploracion_inicial_datos(df):\n",
        "    \"\"\"\n",
        "    Función para realizar una exploración inicial de los datos.\n",
        "\n",
        "    Parámetros:\n",
        "    df (dataframe): El dataframe que contiene los datos.\n",
        "\n",
        "    Muestra las primeras filas, estadísticas descriptivas, valores faltantes,\n",
        "    histogramas de variables numéricas y un mapa de calor de la correlación.\n",
        "    \"\"\"\n",
        "    # Configuración de visualización\n",
        "    sns.set(style=\"whitegrid\")  # Estilo de gráficos\n",
        "\n",
        "\n",
        "    print(\"Primeras filas del DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "\n",
        "    print(\"\\nDescripción estadística de los datos:\")\n",
        "    print(df.describe())\n",
        "\n",
        "\n",
        "    print(\"\\nValores faltantes por columna:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "\n",
        "    print(\"\\nVisualización de histogramas para variables numéricas:\")\n",
        "    df.hist(bins=15, figsize=(15, 10), layout=(5, 4))\n",
        "    plt.show()\n",
        "    print(\"\\nMapa de calor de la matriz de correlación:\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def calcular_layout_optimo(num_cols):\n",
        "    \"\"\"\n",
        "    Calcula el número óptimo de filas y columnas para una figura con subgráficos,\n",
        "    tratando de mantener una forma que sea visualmente agradable y que aproveche el espacio.\n",
        "\n",
        "    Parámetros:\n",
        "    num_cols (int): Número total de columnas (gráficos) a mostrar.\n",
        "\n",
        "    Retorna:\n",
        "    (int, int): Número de filas y columnas para el layout de los subgráficos.\n",
        "    \"\"\"\n",
        "    # Calcula el número óptimo de columnas teniendo un límite visual razonable\n",
        "    cols_per_row = int(np.sqrt(num_cols)) + 1  # Ajuste para maximizar el uso del espacio y la forma de la figura\n",
        "    rows_needed = (num_cols + cols_per_row - 1) // cols_per_row  # Redondeo hacia arriba para incluir todas las columnas\n",
        "    return rows_needed, cols_per_row\n",
        "\n",
        "def exploracion_histogramas(df):\n",
        "    \"\"\"\n",
        "    Función para generar histogramas para todas las columnas numéricas en un DataFrame,\n",
        "    excluyendo las columnas de tipo datetime y no numéricas.\n",
        "\n",
        "    Parámetros:\n",
        "    df (DataFrame): DataFrame de pandas con los datos a analizar.\n",
        "    \"\"\"\n",
        "    # Eliminar columnas no numéricas y de tipo datetime\n",
        "    df_numerico = df.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Número de columnas numéricas\n",
        "    num_cols = df_numerico.shape[1]\n",
        "\n",
        "    # Verificar si hay columnas para mostrar\n",
        "    if num_cols == 0:\n",
        "        print(\"No hay columnas numéricas para mostrar.\")\n",
        "        return\n",
        "\n",
        "    # Calculando el layout necesario\n",
        "    rows_needed, cols_per_row = calcular_layout_optimo(num_cols)\n",
        "\n",
        "    # Crear histogramas\n",
        "    df_numerico.hist(bins=15, figsize=(15, 10), layout=(rows_needed, cols_per_row))\n",
        "    plt.show()\n",
        "\n",
        "def imputar_nan(df):\n",
        "  df.interpolate(method='linear', inplace=True)\n",
        "  df.fillna(method='ffill', inplace=True)\n",
        "  df.fillna(method='bfill', inplace=True)\n",
        "  if sum(df.isnull().sum())!=0:\n",
        "    print(\"Faltan nulos por tratar\")\n",
        "  return df\n",
        "\n",
        "def visualizar_nulos_plot(df, variable_with_nans):\n",
        "    \"\"\"\n",
        "    Plot the specified 'variable_with_nans' column and 'Dst' column from the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pandas.DataFrame containing the data to plot.\n",
        "    - variable_with_nans: str, the name of the column in the DataFrame to plot, which may contain NaNs.\n",
        "\n",
        "    The function assumes that 'Dst' is a column name in the DataFrame and that the DataFrame's index is suitable for plotting (e.g., datetime).\n",
        "    \"\"\"\n",
        "    # Create the figure and subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20, 8))\n",
        "\n",
        "    # Handling NaNs in the 'variable_with_nans' column before plotting\n",
        "    df_plot = df.copy()\n",
        "    df_plot[variable_with_nans] = df_plot[variable_with_nans].fillna(method='ffill')  # Forward fill to handle NaNs\n",
        "\n",
        "    # Plotting 'variable_with_nans' on the first subplot\n",
        "    ax1.scatter(df_plot.index, df_plot[variable_with_nans], label=variable_with_nans, color='blue')\n",
        "    ax1.set_ylabel(variable_with_nans)\n",
        "    ax1.legend(loc='upper right')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plotting 'Dst' on the second subplot\n",
        "    ax2.plot(df_plot.index, df_plot['Dst'], label='Dst', color='red')\n",
        "    ax2.set_ylabel('Dst')\n",
        "    ax2.legend(loc='upper right')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Setting the x-axis label only on the bottom subplot\n",
        "    ax2.set_xlabel('Datetime')\n",
        "\n",
        "    # Improve layout to prevent overlap\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "# Example of how to use this function with a DataFrame containing NaNs\n",
        "data = {\n",
        "    'Datetime': pd.date_range(start='2021-01-01', periods=100, freq='D'),\n",
        "    'variable_with_nans': pd.Series(range(100)).where(lambda x : x % 10 != 0),\n",
        "    'Dst': range(100, 0, -1)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.set_index('Datetime', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "def filter_storms(df, historical_storms, temporal_margin):\n",
        "    \"\"\"\n",
        "    Filter DataFrame entries based on the occurrence of storms within specific time intervals.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): DataFrame containing time-series data with a 'Datetime' column.\n",
        "    historical_storms (pandas.DataFrame): DataFrame containing the start and end times of historical storms.\n",
        "    temporal_margin (int): Number of rows before and after the minimum Dst index to include in the result.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of DataFrame snippets corresponding to the specified storm intervals.\n",
        "    \"\"\"\n",
        "    all_storms = []\n",
        "    for i in range(len(historical_storms)):\n",
        "        df_tmp = df[(df[\"Datetime\"] >= historical_storms.iloc[i][\"start\"]) & (df[\"Datetime\"] <= historical_storms.iloc[i][\"end\"])]\n",
        "        idx = df_tmp['Dst'].idxmin()\n",
        "        all_storms.append(df.iloc[idx-temporal_margin:idx+temporal_margin])\n",
        "    return all_storms\n",
        "\n",
        "def combinar_dataframes_solapados(dfs):\n",
        "    \"\"\"\n",
        "    Combines overlapping DataFrames in a list into non-overlapping DataFrames based on the 'Datetime' column.\n",
        "\n",
        "    Parameters:\n",
        "    dfs (list of pandas.DataFrame): List of DataFrames to combine.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of combined DataFrames without overlap.\n",
        "    \"\"\"\n",
        "    dfs.sort(key=lambda x: x['Datetime'].min())\n",
        "    combinados = []\n",
        "    combinacion_actual = dfs[0]\n",
        "\n",
        "    for df in dfs[1:]:\n",
        "        if df['Datetime'].min() <= combinacion_actual['Datetime'].max():\n",
        "            combinacion_actual = pd.concat([combinacion_actual, df]).drop_duplicates().sort_values(by='Datetime')\n",
        "        else:\n",
        "            combinados.append(combinacion_actual)\n",
        "            combinacion_actual = df\n",
        "    combinados.append(combinacion_actual)\n",
        "    return combinados\n",
        "\n",
        "def scale_data(list_dfs, cols_to_use, col_to_predict):\n",
        "    \"\"\"\n",
        "    Scales columns in a list of DataFrames using StandardScaler.\n",
        "\n",
        "    Parameters:\n",
        "    list_dfs (list of pandas.DataFrame): List of DataFrames to scale.\n",
        "    cols_to_use (list of str): Column names to apply scaling to.\n",
        "    col_to_predict (str): Column name used as a label for prediction.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the list of scaled DataFrames and the label scaler.\n",
        "    \"\"\"\n",
        "    list_dfs_ = []\n",
        "    scaler_cols = StandardScaler()\n",
        "    scaler_label = StandardScaler()\n",
        "    scaler_cols.fit(pd.concat(list_dfs)[cols_to_use])\n",
        "    scaler_label.fit(np.asarray(pd.concat(list_dfs)[col_to_predict]).reshape(-1,1))\n",
        "\n",
        "    for df_ in list_dfs:\n",
        "        df = df_.copy()\n",
        "        df[cols_to_use] = scaler_cols.transform(df[cols_to_use])\n",
        "        list_dfs_.append(df)\n",
        "\n",
        "    return list_dfs_, scaler_label\n",
        "\n",
        "def create_window_df_svr(list_dfs, lookback, lookforward, cols_to_use, col_to_predict, scaler_label=None):\n",
        "    \"\"\"\n",
        "    Creates input and output datasets for SVR training from a list of DataFrames, incorporating windowing and optional descaling for target\n",
        "\n",
        "    Parameters:\n",
        "    list_dfs (list of pandas.DataFrame): List of DataFrames to process.\n",
        "    lookback (int): Number of past records to include as features for each prediction.\n",
        "    lookforward (int): Number of records ahead to predict.\n",
        "    cols_to_use (list of str): List of column names to use as features.\n",
        "    col_to_predict (str): Column name to predict.\n",
        "    scaler_label (StandardScaler, optional): Scaler for the output variable, used for inverse transformation.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing arrays for training features and labels.\n",
        "    \"\"\"\n",
        "    x_train, y_train = [], []\n",
        "\n",
        "    for df_ in list_dfs:\n",
        "        df = df_.copy()\n",
        "\n",
        "        for i in range(len(df) - lookback - lookforward + 1):\n",
        "            x_train.append(np.asarray(df.iloc[i:i+lookback][cols_to_use].values))\n",
        "            y_train.append(np.asarray(df.iloc[i+lookback][col_to_predict]))\n",
        "\n",
        "    if scaler_label is not None:\n",
        "        y_train = scaler_label.inverse_transform(np.asarray(y_train).reshape(-1,1))\n",
        "\n",
        "    return np.asarray(x_train), np.asarray(y_train)\n",
        "\n",
        "def calc_metrics(predictions, y_test):\n",
        "  # Mean Squared Error\n",
        "  mse = mean_squared_error(y_test, predictions)\n",
        "  # Mean Absolute Error\n",
        "  mae = mean_absolute_error(y_test, predictions)\n",
        "  # R^2 Score, the coefficient of determination\n",
        "  r2 = r2_score(y_test, predictions)\n",
        "  # Median Absolute Error\n",
        "  medae = median_absolute_error(y_test, predictions)\n",
        "  # Explained Variance Score\n",
        "  explained_variance = explained_variance_score(y_test, predictions)\n",
        "  # Max Error\n",
        "  max_err = max_error(y_test, predictions)\n",
        "\n",
        "  return mse, mae, r2, medae, explained_variance, max_err\n",
        "\n",
        "\n",
        "def formatear_tiempo(segundos):\n",
        "    horas = int(segundos // 3600)\n",
        "    minutos = int((segundos % 3600) // 60)\n",
        "    segundos = segundos % 60\n",
        "    return f\"{horas} horas, {minutos} minutos, {segundos:.2f} segundos\"\n"
      ],
      "metadata": {
        "id": "MKGLdDwbtVg3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_svr_models(X_train, y_train, X_test, y_test, kernels, C_values, epsilon_values, gamma_values, degree_values, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate SVR models with different hyperparameter configurations and kernel types.\n",
        "\n",
        "    Args:\n",
        "    X_train (array): Independent training data.\n",
        "    y_train (array): Dependent training data (target).\n",
        "    X_test (array): Independent test data.\n",
        "    y_test (array): Dependent test data (target).\n",
        "    kernels (list): List of kernel types to evaluate.\n",
        "    C_values (list): List of values for the penalty parameter C.\n",
        "    epsilon_values (list): List of values for the epsilon parameter.\n",
        "    gamma_values (list): List of values for the gamma parameter.\n",
        "    degree_values (list): List of values for the degree parameter (used only in polynomial kernels).\n",
        "    verbose (bool, optional): If True, prints messages during the evaluation process. Default is True.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A pandas DataFrame containing evaluation metrics for each parameter configuration.\n",
        "    dict: A dictionary of trained models, with keys describing the specific parameter configuration.\n",
        "\n",
        "    Note:\n",
        "    Assumes that the 'precomputed' kernel is only used if the 'precomputed_matrix' is defined in the local environment.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    models_dict = {}\n",
        "\n",
        "    for kernel_ in kernels:\n",
        "        for C in C_values:\n",
        "            for epsilon in epsilon_values:\n",
        "                # Check parameter relevance for gamma and degree for the current kernel\n",
        "                relevant_gamma_values = gamma_values if kernel_ in ['rbf', 'sigmoid', 'poly'] else [None]\n",
        "                relevant_degree_values = degree_values if kernel_ == 'poly' else [None]\n",
        "\n",
        "                for gamma in relevant_gamma_values:\n",
        "                    for degree in relevant_degree_values:\n",
        "                        start = time.time()\n",
        "                        config_key = f\"{kernel_}_C{C}_eps{epsilon}_gamma{gamma}_deg{degree}\"\n",
        "                        if verbose:\n",
        "                            print(f\"Starting {config_key}\")\n",
        "\n",
        "                        if kernel_ == \"precomputed\":\n",
        "                            if 'precomputed_matrix' in locals():\n",
        "                                model = SVR(kernel='precomputed', C=C, epsilon=epsilon)\n",
        "                                model.fit(precomputed_matrix, y_train)\n",
        "                                models_dict[config_key] = model\n",
        "                                continue\n",
        "                            else:\n",
        "                                if verbose:\n",
        "                                    print(\"Precomputed matrix not defined for kernel='precomputed'\")\n",
        "                                continue\n",
        "\n",
        "                        model = SVR(kernel=kernel_, C=C, epsilon=epsilon, gamma=(gamma if gamma is not None else 0.01), degree=(degree if degree is not None else 3))\n",
        "                        model.fit(X_train, y_train)\n",
        "                        models_dict[config_key] = model\n",
        "\n",
        "                        predictions = model.predict(X_test)\n",
        "                        mse, mae, r2, medae, explained_variance, max_err = calc_metrics(predictions, y_test)\n",
        "                        end = time.time()\n",
        "                        time_exec = formatear_tiempo(end - start)\n",
        "\n",
        "                        results.append({\n",
        "                            \"kernel\": kernel_,\n",
        "                            \"C\": C,\n",
        "                            \"epsilon\": epsilon,\n",
        "                            \"gamma\": gamma,\n",
        "                            \"degree\": degree,\n",
        "                            \"mse\": mse,\n",
        "                            \"mae\": mae,\n",
        "                            \"r2\": r2,\n",
        "                            \"medae\": medae,\n",
        "                            \"exp_var\": explained_variance,\n",
        "                            \"max_err\": max_err,\n",
        "                            \"time_exec\": time_exec\n",
        "                        })\n",
        "\n",
        "                        print(f\"{config_key} finished. Time for iteration: {time_exec} | mae: {mae} | mse: {mse} | exp_var: {explained_variance} | max_err: {max_err}\")\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    return df_results, models_dict\n"
      ],
      "metadata": {
        "id": "dhsyk8pnCibA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Carga y preparación de datos"
      ],
      "metadata": {
        "id": "IiZeUiNVpNrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Carga de los datos"
      ],
      "metadata": {
        "id": "Kt5CN8t9pnMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hd = pd.read_csv(tfm_path+H1_code+'.csv', parse_dates=[\"Datetime\"])\n",
        "# md = pd.read_csv(tfm_path+M5_code+'.csv', parse_dates=[\"Datetime\"]) # no se va a usar por ahora\n",
        "historical_storms = pd.read_csv(tfm_path+hstorms_data)\n",
        "# historical_storms = historical_storms.drop(columns=['Min. Dst (nT)','Unnamed: 0'], axis=1)"
      ],
      "metadata": {
        "id": "o0zK7ahApmQf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ver qué columnas y tipo de datos contienen los df."
      ],
      "metadata": {
        "id": "w4mHmDLrtEKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hd.columns"
      ],
      "metadata": {
        "id": "up24FqxwscRS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7710bfc8-a07d-4d3c-d66f-03f02ebcc41b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID_IMF', 'ID_plasma', 'Bmag', 'dev_Bmag', 'Bx', 'By_gse', 'Bz_gse',\n",
              "       'By_gsm', 'Bz_gsm', 'dev_Bx', 'dev_By', 'dev_Bz', 'P_density',\n",
              "       'dev_P_density', 'AP', 'dev_AP', 'E_field', 'plasma_T', 'dev_plasma_T',\n",
              "       'plasma_V', 'Dst', 'Datetime'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hd.dtypes"
      ],
      "metadata": {
        "id": "f3lGenT5tId9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1532dfe-95d6-4d96-9b1b-06ef5d627681"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID_IMF                  float64\n",
              "ID_plasma               float64\n",
              "Bmag                    float64\n",
              "dev_Bmag                float64\n",
              "Bx                      float64\n",
              "By_gse                  float64\n",
              "Bz_gse                  float64\n",
              "By_gsm                  float64\n",
              "Bz_gsm                  float64\n",
              "dev_Bx                  float64\n",
              "dev_By                  float64\n",
              "dev_Bz                  float64\n",
              "P_density               float64\n",
              "dev_P_density           float64\n",
              "AP                      float64\n",
              "dev_AP                  float64\n",
              "E_field                 float64\n",
              "plasma_T                float64\n",
              "dev_plasma_T            float64\n",
              "plasma_V                float64\n",
              "Dst                     float64\n",
              "Datetime         datetime64[ns]\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "historical_storms.columns"
      ],
      "metadata": {
        "id": "2s-_PYArsgz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c215c8-d879-4912-bd00-293866e69887"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Min. Dst (nT)', 'start', 'end', 'storm'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "historical_storms.dtypes"
      ],
      "metadata": {
        "id": "oq6T4hJDsm3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca971781-297f-4ec0-f5f8-0fd4a87c0f2b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0        int64\n",
              "Min. Dst (nT)     int64\n",
              "start            object\n",
              "end              object\n",
              "storm             int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ordenar las tormentas en caso de que no lo estén\n",
        "- Convertir todas las fechas a datetime de pd."
      ],
      "metadata": {
        "id": "jyL09t1Ks9jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordenar los dataframes por fecha\n",
        "historical_storms = historical_storms.sort_values(by='start')\n",
        "\n",
        "# convertir las columnas de tiempo a datetime64\n",
        "hd['Datetime']=pd.to_datetime(hd['Datetime'])\n",
        "historical_storms['start']=pd.to_datetime(historical_storms['start'])\n",
        "historical_storms['end']=pd.to_datetime(historical_storms['end'])\n",
        "\n",
        "# Cuando se utilizan los datos a 5 minutos, se unen a 5min\n",
        "#data = pd.merge(md, hd[[\"Datetime\", \"Dst\"]], on='Datetime', how='left')"
      ],
      "metadata": {
        "id": "KKh0YUUIq6xc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Exploración inicial de los datos"
      ],
      "metadata": {
        "id": "VzCZp1Ndpt_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- head del df"
      ],
      "metadata": {
        "id": "Sngrxbi1uXRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hd.head()"
      ],
      "metadata": {
        "id": "lavp1YMbuOQZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- descripción estadística de los datos"
      ],
      "metadata": {
        "id": "IVAqYfC9uU3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hd.describe()"
      ],
      "metadata": {
        "id": "t0-OstdCuSlD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ver los valores nulos"
      ],
      "metadata": {
        "id": "v9Ry8TktulyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hd.isnull().sum()"
      ],
      "metadata": {
        "id": "xnw_H2T-ulg5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hd.isnull().sum() / len(hd) * 100"
      ],
      "metadata": {
        "id": "d-LN6q6fuixi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Histograma para variables numéricas (todas, en este caso, pero hemos quitado datetime porque no es muy util al ser una serie temporal)."
      ],
      "metadata": {
        "id": "Jx6zks84uzv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exploracion_histogramas(hd)"
      ],
      "metadata": {
        "id": "ke7ogtCzu0KB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Visualizaciónd e historical_storms"
      ],
      "metadata": {
        "id": "gkDpVZ15xPMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storms_data = historical_storms.copy()"
      ],
      "metadata": {
        "id": "Vr2COnTg0D6S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convertir las columnas de fecha y hora a datetime\n",
        "# storms_data['start'] = pd.to_datetime(storms_data['start'])\n",
        "# storms_data['end'] = pd.to_datetime(storms_data['end'])\n",
        "# # Calcular la duración de cada tormenta en horas\n",
        "# storms_data['duration_hours'] = (storms_data['end'] - storms_data['start']).dt.total_seconds() / 3600\n",
        "\n",
        "# # Gráfico de la distribución de las intensidades mínimas de las tormentas\n",
        "# plt.figure(figsize=(8, 4))\n",
        "# sns.histplot(storms_data['Min. Dst (nT)'], bins=30, kde=True, color='blue')\n",
        "# plt.title('Distribución de Intensidades Mínimas de Tormentas Geomagnéticas')\n",
        "# plt.xlabel('Min. Dst (nT)')\n",
        "# plt.ylabel('Frecuencia')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "4ivSRmTdz-Kf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico de dispersión entre la duración y la intensidad mínima"
      ],
      "metadata": {
        "id": "tnkxO8Zk0pnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(12, 6))\n",
        "# sns.scatterplot(x='duration_hours', y='Min. Dst (nT)', data=storms_data)\n",
        "# plt.title('Correlación entre Duración e Intensidad Mínima de Tormentas')\n",
        "# plt.xlabel('Duración (Horas)')\n",
        "# plt.ylabel('Min. Dst (nT)')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "hCSpRC9N0i8E"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ver el timeline de las tormentas"
      ],
      "metadata": {
        "id": "-2tC7isT0yJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import plotly.graph_objects as go\n",
        "\n",
        "# # Crear la figura\n",
        "# fig = go.Figure()\n",
        "\n",
        "# # Añadir cada tormenta como un segmento de línea en el gráfico\n",
        "# for i, row in storms_data.iterrows():\n",
        "#     fig.add_trace(go.Scatter(\n",
        "#         x=[row['start'], row['end']],\n",
        "#         y=[i, i],\n",
        "#         mode='lines+markers',\n",
        "#         name=f\"Tormenta {row['storm']}\"\n",
        "#     ))\n",
        "\n",
        "# # Ajustar la presentación de la figura\n",
        "# fig.update_layout(\n",
        "#     title=\"Línea de Tiempo de Tormentas Históricas\",\n",
        "#     xaxis_title=\"Fecha\",\n",
        "#     yaxis_title=\"Tormenta\",\n",
        "#     yaxis=dict(\n",
        "#         tickmode='array',\n",
        "#         tickvals=list(range(len(storms_data)))\n",
        "#     ),\n",
        "#     showlegend=False,\n",
        "#     xaxis=dict(\n",
        "#         rangeslider=dict(\n",
        "#             visible=True\n",
        "#         ),\n",
        "#         type=\"date\"\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# # Mostrar la figura\n",
        "# fig.show()"
      ],
      "metadata": {
        "id": "KfIV8r9Q00gx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapa de calor de la matriz de correlación"
      ],
      "metadata": {
        "id": "-ABxa-pMvUiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(15, 8))\n",
        "# sns.heatmap(hd.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ttbLXb7UvWm0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Limpieza y procesamiento de los datos"
      ],
      "metadata": {
        "id": "1-5Cz91Ap3RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 Tratamiento de valores faltantes"
      ],
      "metadata": {
        "id": "D4RKW9php63d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(hd.isnull().sum())"
      ],
      "metadata": {
        "id": "t7usCL6U8a6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdcc39a1-9684-4ead-8e4b-285780a05c21"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42472"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hd = imputar_nan(hd)"
      ],
      "metadata": {
        "id": "FBM6gfRL85Tb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(hd.isnull().sum())"
      ],
      "metadata": {
        "id": "AwfFb2yZ8-dU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36a4b2a-ddae-4722-c120-2b2ca1b019d3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 Normalización de los datos"
      ],
      "metadata": {
        "id": "yn3SwS3Np93N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_storms = filter_storms(hd, historical_storms, temporal_margin)\n",
        "all_storms = combinar_dataframes_solapados(all_storms)"
      ],
      "metadata": {
        "id": "tSl0lkmfrHEz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat(all_storms).to_csv(tfm_path+\"all_storms.csv\")"
      ],
      "metadata": {
        "id": "z9Fj84tVSCwr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storms_check = pd.concat(all_storms)"
      ],
      "metadata": {
        "id": "duerUHXhIFa5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_storms, scaler_target = scale_data(all_storms, cols_to_use, col_to_predict)"
      ],
      "metadata": {
        "id": "dWxN6o_sIFsf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 Codificación de variables categóricas\n",
        "\n",
        "* No existe codificación de variables categóricas porque **no hay variables categóricas). A continuación, se muestra."
      ],
      "metadata": {
        "id": "QSZtUWlLqCAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat(all_storms).dtypes"
      ],
      "metadata": {
        "id": "Xwu8MOV3qb2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d495a7-a9a3-4872-82e7-d1563a0e7d80"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID_IMF                  float64\n",
              "ID_plasma               float64\n",
              "Bmag                    float64\n",
              "dev_Bmag                float64\n",
              "Bx                      float64\n",
              "By_gse                  float64\n",
              "Bz_gse                  float64\n",
              "By_gsm                  float64\n",
              "Bz_gsm                  float64\n",
              "dev_Bx                  float64\n",
              "dev_By                  float64\n",
              "dev_Bz                  float64\n",
              "P_density               float64\n",
              "dev_P_density           float64\n",
              "AP                      float64\n",
              "dev_AP                  float64\n",
              "E_field                 float64\n",
              "plasma_T                float64\n",
              "dev_plasma_T            float64\n",
              "plasma_V                float64\n",
              "Dst                     float64\n",
              "Datetime         datetime64[ns]\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Entrenamiento de modelos"
      ],
      "metadata": {
        "id": "ULHHGwCjqr6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Creación de la ventana temporal"
      ],
      "metadata": {
        "id": "7zqWtTvA4Ycl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_window, y_window = create_window_df_svr(all_storms, lookforward, cols_to_use, col_to_predict, scaler_target) # desescalado\n",
        "x_window, y_window = create_window_df_svr(all_storms, lookback, lookforward, cols_to_use, col_to_predict) # sin desescalar"
      ],
      "metadata": {
        "id": "XAm8sTsD4WrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 División en train-test."
      ],
      "metadata": {
        "id": "WWNESFbg5L_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_window, y_window, test_size=test_size, random_state=42)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)  # Transforma a (n_samples, n_features*lookback)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)  # Lo mismo para el conjunto de prueba\n",
        "#X_train = np.squeeze(X_train, axis=1)\n",
        "#X_test = np.squeeze(X_test, axis=1)"
      ],
      "metadata": {
        "id": "GIqtYB085ONF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Entrenamiento de SVR"
      ],
      "metadata": {
        "id": "9cp3vnX99IhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir los límites en log10 para gamma y C\n",
        "start = -6\n",
        "end = 2\n",
        "\n",
        "# Número de puntos más reducido\n",
        "num_points = 3  # menos puntos para una exploración más ágil\n",
        "\n",
        "# Generar valores de gamma y C en escala logarítmica\n",
        "gamma_values = np.logspace(start, end, num=num_points, base=10)\n",
        "C_values = np.logspace(start, end, num=num_points, base=10)\n",
        "\n",
        "# Intervalo más amplio y menos puntos para epsilon\n",
        "epsilon_values = np.linspace(0.001, 0.1, num=5)\n",
        "\n",
        "# Reducir el rango de grados para el kernel polinomial\n",
        "degree_values = np.arange(3, 5)  # considera grados de 1 a 4"
      ],
      "metadata": {
        "id": "XedppVeOln3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_results, models_result = evaluate_svr_models(X_train, y_train, X_test, y_test, kernels, C_values, epsilon_values, gamma_values, degree_values, verbose=False)"
      ],
      "metadata": {
        "id": "ep5t-bWq9tYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Optimización de la selección de hiperparámetros"
      ],
      "metadata": {
        "id": "_2EUAcFYM2ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    # Suggesting hyperparameters using suggest_float\n",
        "    C = trial.suggest_float('C', 1e-6, 1e2, log=True)  # Logarithmic for C\n",
        "    gamma = trial.suggest_float('gamma', 1e-6, 1e2, log=True)  # Logarithmic for gamma\n",
        "    epsilon = trial.suggest_float('epsilon', 0.001, 0.1)  # Linear for epsilon\n",
        "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n",
        "\n",
        "    # Create and train the SVR model from Thunder SVM\n",
        "    model = SVR(C=C, gamma=gamma, epsilon=epsilon, kernel=kernel)\n",
        "\n",
        "    # Evaluate the model using cross-validation\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error').mean()\n",
        "    return -score"
      ],
      "metadata": {
        "id": "9xV7WmtdM5p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the full path to the SQLite database\n",
        "database_path = os.path.join(tfm_path, 'study1.db')\n",
        "storage_url = f'sqlite:///{database_path}'"
      ],
      "metadata": {
        "id": "Mu_n8y_FSCs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Evaluación de modelos"
      ],
      "metadata": {
        "id": "OJ-V-0cQqtmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(study_name=\"first_study_50_noGPUacceleration\", storage=storage_url, load_if_exists=True)\n",
        "study.optimize(objective, n_trials=50)  # Puedes ajustar el número de trials\n",
        "\n",
        "# Mostrar los mejores hiperparámetros\n",
        "print('Mejores hiperparámetros:', study.best_params)\n"
      ],
      "metadata": {
        "id": "eiXG6EvmkVli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = study.best_params\n",
        "final_model = SVR(**best_params)\n",
        "final_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "TF3kvcJfNeBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que X_test incluye una columna 'datetime' que fue eliminada antes del entrenamiento\n",
        "dates_test = pd.to_datetime(df_test['datetime'])  # Asegúrate de que sean tipos datetime\n",
        "\n",
        "# Filtrar fechas por tormenta\n",
        "for _, storm in df_storms.iterrows():\n",
        "    start, end = storm['start'], storm['end']\n",
        "    mask = (dates_test >= start) & (dates_test <= end)\n",
        "    dates_storm = dates_test[mask]\n",
        "    y_pred_storm = y_pred[mask]\n",
        "\n",
        "    # Visualizar los resultados para esta tormenta\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(dates_storm, y_pred_storm, label='Predicción desescalada')\n",
        "    plt.title(f\"Predicciones para la tormenta desde {start} hasta {end}\")\n",
        "    plt.xlabel('Fecha y Hora')\n",
        "    plt.ylabel('Valor Predicho')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6woG3xo0N-LX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
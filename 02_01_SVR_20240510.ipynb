{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raqgmar/tsa4dst/blob/main/02_01_SVR_20240510.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xussTphZSYVz"
      },
      "outputs": [],
      "source": [
        "tfm_path='/content/drive/MyDrive/TFM data/'\n",
        "H1_code = 'OMNI2_H0_MRG1HR'\n",
        "M5_code = 'OMNI_HRO2_5MIN'\n",
        "lookback = 24\n",
        "lookforward = 12\n",
        "cols_to_use = ['Bx', 'By_gse', 'Bz_gse', 'By_gsm', 'Bz_gsm', 'P_density', 'AP', 'E_field', 'plasma_T', 'plasma_V']\n",
        "col_to_predict = \"Dst\"\n",
        "hstorms_data = 'historical_storms_gruet2018.csv'\n",
        "weak_threshold = -30 #1\n",
        "moderate_threshold = -50 #2\n",
        "strong_threshold = -100 #3\n",
        "severe_threshold = -200 #4\n",
        "great_threshold = -300 #5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UqjKD-bTrXW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2tXbl93Tndp",
        "outputId": "7c26106c-35b5-418c-ba9b-8091fdbccaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!rm -rf sample_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cSHymDYgTpV3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# métricas\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import mean_squared_log_error, median_absolute_error\n",
        "from sklearn.metrics import explained_variance_score, max_error\n",
        "\n",
        "\n",
        "# timer\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tyJlxumkTLwN"
      },
      "outputs": [],
      "source": [
        "hd = pd.read_csv(tfm_path+H1_code+'.csv', parse_dates=[\"Datetime\"])\n",
        "# md = pd.read_csv(tfm_path+M5_code+'.csv', parse_dates=[\"Datetime\"])\n",
        "# historical_storms = pd.read_csv(tfm_path+hstorms_data)\n",
        "# historical_storms = historical_storms.drop(columns=['Min. Dst (nT)','Unnamed: 0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xcGWrr0QUHkf"
      },
      "outputs": [],
      "source": [
        "# # Ordenar los dataframes por fecha\n",
        "# historical_storms = historical_storms.sort_values(by='start')\n",
        "# hd['Datetime']=pd.to_datetime(hd['Datetime'])\n",
        "# historical_storms['start']=pd.to_datetime(historical_storms['start'])\n",
        "# historical_storms['end']=pd.to_datetime(historical_storms['end'])\n",
        "# # Unir basado en la condición de fecha\n",
        "hd = pd.merge_asof(hd.sort_values('Datetime'), historical_storms, left_on='Datetime', right_on='start', direction='forward')\n",
        "\n",
        "# # Filtrar para asegurar que la fecha de 'hmd' está dentro del intervalo start y end\n",
        "# hd['storm'] = hd.apply(lambda row: row['storm'] if row['Datetime'] <= row['end'] else None, axis=1)\n",
        "\n",
        "# # Limpiar el DataFrame resultante si es necesario\n",
        "# hd = hd.drop(columns=['start', 'end'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OgL1NvYxbI8P"
      },
      "outputs": [],
      "source": [
        "#data = pd.merge(md, hd[[\"Datetime\", \"Dst\"]], on='Datetime', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = hm"
      ],
      "metadata": {
        "id": "RkP28090pZYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ya tenemos un pandas dataframe: str(type(data))=\"+str(type(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j74zvRSvmsOl",
        "outputId": "3bcf4407-39f3-4be5-9679-a796826893b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ya tenemos un pandas dataframe: str(type(data))=<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input X contains NaN. Por tanto, tenemos que hacer imputación de nulos.\n",
        "\n",
        "## Imputación de valores nulos"
      ],
      "metadata": {
        "id": "InkskSyKm8Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = len(data)\n",
        "null_counts = data.isnull().sum()\n",
        "null_percentage = (null_counts / total_rows) * 100\n",
        "print(null_percentage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdAzzzu-m70I",
        "outputId": "4397d0ac-6294-471f-b206-0c5f317a927d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID_IMF        5.679015\n",
            "ID_plasma     8.763609\n",
            "Bx            5.679015\n",
            "By_gse        5.679015\n",
            "Bz_gse        5.679015\n",
            "By_gsm        5.679015\n",
            "Bz_gsm        5.679015\n",
            "P_density     8.763609\n",
            "AP           61.723860\n",
            "E_field       8.965980\n",
            "plasma_T      8.787445\n",
            "plasma_V      8.763609\n",
            "Datetime      0.000000\n",
            "Dst          91.666667\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_rows)"
      ],
      "metadata": {
        "id": "Tt9AJg4qpg2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop('AP',axis=1)"
      ],
      "metadata": {
        "id": "RfGXMTywpEsW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_use.remove('AP')"
      ],
      "metadata": {
        "id": "7-4SVBmXpzBA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.interpolate(method='linear', inplace=True)\n",
        "\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nOfxxcdpKRA",
        "outputId": "85c821a5-4918-42c5-bc54-29ddff2fb5fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID_IMF       0\n",
            "ID_plasma    0\n",
            "Bx           0\n",
            "By_gse       0\n",
            "Bz_gse       0\n",
            "By_gsm       0\n",
            "Bz_gsm       0\n",
            "P_density    0\n",
            "E_field      0\n",
            "plasma_T     0\n",
            "plasma_V     0\n",
            "Datetime     0\n",
            "Dst          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0o_DIJFbcwR"
      },
      "source": [
        "## Obtain train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UNsBlaEfZ_HL"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "X = data[cols_to_use]\n",
        "y = data[col_to_predict]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train)"
      ],
      "metadata": {
        "id": "hZQGKW7RpzDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_window_df_svr(df):\n",
        "  df = df.copy()\n",
        "  df[col_to_predict] = df[col_to_predict].shift(lookforward)\n",
        "  x_train, y_train = [], []\n",
        "\n",
        "  for index in df.dropna(subset=col_to_predict).index:\n",
        "    if index >= lookback:\n",
        "      x_train.append(np.asarray(df.iloc[index-lookback:index][cols_to_use].values))\n",
        "      y_train.append(np.asarray(df.iloc[index][col_to_predict]))\n",
        "      np.asarray(x_train), np.asarray(y_train)\n",
        "  return data.reshape(data.shape[0], -1)"
      ],
      "metadata": {
        "id": "K0MkeCmepx9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ek5SurelkjhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_metrics(predictions, y_test):\n",
        "  # Mean Squared Error\n",
        "  mse = mean_squared_error(y_test, predictions)\n",
        "  # Mean Absolute Error\n",
        "  mae = mean_absolute_error(y_test, predictions)\n",
        "  # R^2 Score, the coefficient of determination\n",
        "  r2 = r2_score(y_test, predictions)\n",
        "  # Mean Squared Logarithmic Error\n",
        "  msle = mean_squared_log_error(y_test, predictions)\n",
        "  # Median Absolute Error\n",
        "  medae = median_absolute_error(y_test, predictions)\n",
        "  # Explained Variance Score\n",
        "  explained_variance = explained_variance_score(y_test, predictions)\n",
        "  # Max Error\n",
        "  max_err = max_error(y_test, predictions)\n",
        "\n",
        "  return mse, mae, r2, msle, medae, explained_variance, max_err\n"
      ],
      "metadata": {
        "id": "B36Q6NkrtUvy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatear_tiempo(segundos):\n",
        "    horas = int(segundos // 3600)\n",
        "    minutos = int((segundos % 3600) // 60)\n",
        "    segundos = segundos % 60\n",
        "    return f\"{horas} horas, {minutos} minutos, {segundos:.2f} segundos\""
      ],
      "metadata": {
        "id": "fiLQD-WqzcgN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear"
      ],
      "metadata": {
        "id": "lFchnF3niVUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "mse_list = []\n",
        "mae_list = []\n",
        "r2_list = []\n",
        "msle_list = []\n",
        "medae_list = []\n",
        "exp_var_list = []\n",
        "max_err_list = []\n",
        "time_list = []\n",
        "\n",
        "for kernel_ in kernels:\n",
        "  start = time.time()\n",
        "  print(\"Starting {}\".format(kernel_))\n",
        "  if kernel_ == \"linear\":\n",
        "    model = SVR(kernel='linear', C=1.0, epsilon=0.1, gamma='auto')\n",
        "  elif kernel_ == \"poly\":\n",
        "    model = SVR(kernel='poly', C=1.0, epsilon=0.1, gamma='auto', degree=3)  # Degree is optional, default is 3\n",
        "  elif kernel_ == \"rbf\":\n",
        "    model = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='auto')\n",
        "  elif kernel_ == \"sigmoid\":\n",
        "    model = SVR(kernel='sigmoid', C=1.0, epsilon=0.1, gamma='auto')\n",
        "  elif kernel_ == \"precomputed\":\n",
        "    # Note: For kernel='precomputed', you need to supply a precomputed kernel matrix instead of X_train\n",
        "    # Here's a placeholder assuming precomputed_matrix is already defined\n",
        "    model = SVR(kernel='precomputed', C=1.0, epsilon=0.1)\n",
        "    if 'precomputed_matrix' in locals():\n",
        "      model.fit(precomputed_matrix, y_train)\n",
        "      continue  # Skip the rest of the loop if using precomputed kernel\n",
        "    else:\n",
        "      print(\"Precomputed matrix not defined for kernel='precomputed'\")\n",
        "      continue\n",
        "\n",
        "  # Fit the model on training data\n",
        "  model.fit(X_train, y_train)\n",
        "  # Prediction and Evaluation\n",
        "  predictions = model.predict(X_test)\n",
        "\n",
        "  mse, mae, r2, msle, medae, explained_variance, max_err = calc_metrics(predictions, y_test)\n",
        "  mse_list.append(mse)\n",
        "  mae_list.append(mae)\n",
        "  r2_list.append(r2)\n",
        "  msle_list.append(msle)\n",
        "  medae_list.append(medae)\n",
        "  exp_var_list.append(explained_variance)\n",
        "  max_err_list.append(max_err)\n",
        "\n",
        "  end = time.time()\n",
        "  time_exec = formatear_tiempo(end-start)\n",
        "  time_list.append(time_exec)\n",
        "  print(\"{} finished. Time for iteration: {}\".format(kernel_, time_exec))\n",
        "\n",
        "\n",
        "\n",
        "df_results = pd.DataFrame(\n",
        "    {\n",
        "        \"kernel\": kernels,\n",
        "        \"mse\": mse_list,\n",
        "        \"mae\": mae_list,\n",
        "        \"r2\": r2_list,\n",
        "        \"msle\": msle_list,\n",
        "        \"medae\": medae_list,\n",
        "        \"exp_var\": exp_var_list,\n",
        "        \"max_err\": max_err_list,\n",
        "        \"time_exec\": time_list\n",
        "    }\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIc6yCKgiU-m",
        "outputId": "929736db-03c8-436d-c7aa-c855cdc75906"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting linear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results"
      ],
      "metadata": {
        "id": "NT6hNnqGxhBW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
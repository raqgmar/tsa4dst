{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHP5VOu0oiCD"
   },
   "source": [
    "# 0 Preparación del entorno.\n",
    "\n",
    "## 0.1 Definición de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xussTphZSYVz"
   },
   "outputs": [],
   "source": [
    "tfm_path='C:/Users/raqga/OneDrive - Universidad Complutense de Madrid (UCM)/Documentos/tsa4dst/TFM_data/'\n",
    "H1_code = 'OMNI2_H0_MRG1HR'\n",
    "lookback = 12\n",
    "lookforward = 2\n",
    "tfm_path_Nh_models = f'PRED_{lookforward}h/'\n",
    "cols_to_use = ['Bx', 'By_gse', 'Bz_gse', 'By_gsm', 'Bz_gsm', 'P_density', 'E_field', 'plasma_T', 'plasma_V', 'Dst'] # 'AP', out\n",
    "col_to_pred = \"Dst\"\n",
    "hstorms_data = 'historical_storms_gruet2018.csv'\n",
    "weak_threshold = -30 #1\n",
    "moderate_threshold = -50 #2\n",
    "strong_threshold = -100 #3\n",
    "severe_threshold = -200 #4\n",
    "great_threshold = -300 #5\n",
    "gamma_value=0.0001\n",
    "temporal_margin=5*24 # margen para obtener tiempos ampliados de las tormentas de gruet et al 2018\n",
    "test_size = 0.2\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_NN_RES_BY_STORM = \"SVR_{}h_res_by_storm/storm_{}.csv\"\n",
    "PATH_NN_RES_BY_STORM_PLOTS = \"SVR_{}h_res_by_storm/storm_{}.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTnEx3feo7e4"
   },
   "source": [
    "## 0.2 Montar Google Drive (obtención de datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n2tXbl93Tndp"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !rm -rf sample_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNzRT2z-o-ps"
   },
   "source": [
    "## 0.3 Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32213,
     "status": "ok",
     "timestamp": 1719831629912,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "OJtQv_kvy3ky",
    "outputId": "23d7858f-5abc-4373-fdb2-42842871bbe0"
   },
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3792,
     "status": "ok",
     "timestamp": 1719831633701,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "sJuDqU5SzL4m"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# librerías de manipulación de datos y gráficos\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "\n",
    "# gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# modelo\n",
    "from sklearn.svm import SVR\n",
    "#from thundersvm import SVR\n",
    "# escalado y división en train/test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# obtención de métricas\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_log_error, median_absolute_error\n",
    "from sklearn.metrics import explained_variance_score, max_error\n",
    "\n",
    "# meta\n",
    "# timer\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cUAiVDJpFBD"
   },
   "source": [
    "## 0.4 Definición de funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1719831633702,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "MKGLdDwbtVg3"
   },
   "outputs": [],
   "source": [
    "def exploracion_inicial_datos(df):\n",
    "    \"\"\"\n",
    "    Función para realizar una exploración inicial de los datos.\n",
    "\n",
    "    Parámetros:\n",
    "    df (dataframe): El dataframe que contiene los datos.\n",
    "\n",
    "    Muestra las primeras filas, estadísticas descriptivas, valores faltantes,\n",
    "    histogramas de variables numéricas y un mapa de calor de la correlación.\n",
    "    \"\"\"\n",
    "    # Configuración de visualización\n",
    "    sns.set(style=\"whitegrid\")  # Estilo de gráficos\n",
    "\n",
    "\n",
    "    print(\"Primeras filas del DataFrame:\")\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "    print(\"\\nDescripción estadística de los datos:\")\n",
    "    print(df.describe())\n",
    "\n",
    "\n",
    "    print(\"\\nValores faltantes por columna:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "\n",
    "    print(\"\\nVisualización de histogramas para variables numéricas:\")\n",
    "    df.hist(bins=15, figsize=(15, 10), layout=(5, 4))\n",
    "    plt.show()\n",
    "    print(\"\\nMapa de calor de la matriz de correlación:\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calcular_layout_optimo(num_cols):\n",
    "    \"\"\"\n",
    "    Calcula el número óptimo de filas y columnas para una figura con subgráficos,\n",
    "    tratando de mantener una forma que sea visualmente agradable y que aproveche el espacio.\n",
    "\n",
    "    Parámetros:\n",
    "    num_cols (int): Número total de columnas (gráficos) a mostrar.\n",
    "\n",
    "    Retorna:\n",
    "    (int, int): Número de filas y columnas para el layout de los subgráficos.\n",
    "    \"\"\"\n",
    "    # Calcula el número óptimo de columnas teniendo un límite visual razonable\n",
    "    cols_per_row = int(np.sqrt(num_cols)) + 1  # Ajuste para maximizar el uso del espacio y la forma de la figura\n",
    "    rows_needed = (num_cols + cols_per_row - 1) // cols_per_row  # Redondeo hacia arriba para incluir todas las columnas\n",
    "    return rows_needed, cols_per_row\n",
    "\n",
    "def exploracion_histogramas(df):\n",
    "    \"\"\"\n",
    "    Función para generar histogramas para todas las columnas numéricas en un DataFrame,\n",
    "    excluyendo las columnas de tipo datetime y no numéricas.\n",
    "\n",
    "    Parámetros:\n",
    "    df (DataFrame): DataFrame de pandas con los datos a analizar.\n",
    "    \"\"\"\n",
    "    # Eliminar columnas no numéricas y de tipo datetime\n",
    "    df_numerico = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Número de columnas numéricas\n",
    "    num_cols = df_numerico.shape[1]\n",
    "\n",
    "    # Verificar si hay columnas para mostrar\n",
    "    if num_cols == 0:\n",
    "        print(\"No hay columnas numéricas para mostrar.\")\n",
    "        return\n",
    "\n",
    "    # Calculando el layout necesario\n",
    "    rows_needed, cols_per_row = calcular_layout_optimo(num_cols)\n",
    "\n",
    "    # Crear histogramas\n",
    "    df_numerico.hist(bins=15, figsize=(15, 10), layout=(rows_needed, cols_per_row))\n",
    "    plt.show()\n",
    "\n",
    "def imputar_nan(df):\n",
    "  df.interpolate(method='linear', inplace=True)\n",
    "  df.fillna(method='ffill', inplace=True)\n",
    "  df.fillna(method='bfill', inplace=True)\n",
    "  if sum(df.isnull().sum())!=0:\n",
    "    print(\"Faltan nulos por tratar\")\n",
    "  return df\n",
    "\n",
    "def visualizar_nulos_plot(df, variable_with_nans):\n",
    "    \"\"\"\n",
    "    Plot the specified 'variable_with_nans' column and 'Dst' column from the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame containing the data to plot.\n",
    "    - variable_with_nans: str, the name of the column in the DataFrame to plot, which may contain NaNs.\n",
    "\n",
    "    The function assumes that 'Dst' is a column name in the DataFrame and that the DataFrame's index is suitable for plotting (e.g., datetime).\n",
    "    \"\"\"\n",
    "    # Create the figure and subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20, 8))\n",
    "\n",
    "    # Handling NaNs in the 'variable_with_nans' column before plotting\n",
    "    df_plot = df.copy()\n",
    "    df_plot[variable_with_nans] = df_plot[variable_with_nans].fillna(method='ffill')  # Forward fill to handle NaNs\n",
    "\n",
    "    # Plotting 'variable_with_nans' on the first subplot\n",
    "    ax1.scatter(df_plot.index, df_plot[variable_with_nans], label=variable_with_nans, color='blue')\n",
    "    ax1.set_ylabel(variable_with_nans)\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plotting 'Dst' on the second subplot\n",
    "    ax2.plot(df_plot.index, df_plot['Dst'], label='Dst', color='red')\n",
    "    ax2.set_ylabel('Dst')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Setting the x-axis label only on the bottom subplot\n",
    "    ax2.set_xlabel('Datetime')\n",
    "\n",
    "    # Improve layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example of how to use this function with a DataFrame containing NaNs\n",
    "data = {\n",
    "    'Datetime': pd.date_range(start='2021-01-01', periods=100, freq='D'),\n",
    "    'variable_with_nans': pd.Series(range(100)).where(lambda x : x % 10 != 0),\n",
    "    'Dst': range(100, 0, -1)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('Datetime', inplace=True)\n",
    "\n",
    "def create_window_df_svr(list_dfs, lookback, lookforward, cols_to_use, col_to_pred, scaler_label=None):\n",
    "    \"\"\"\n",
    "    Creates input and output datasets for SVR training from a list of DataFrames, incorporating windowing and optional descaling for target\n",
    "\n",
    "    Parameters:\n",
    "    list_dfs (list of pandas.DataFrame): List of DataFrames to process.\n",
    "    lookback (int): Number of past records to include as features for each prediction.\n",
    "    lookforward (int): Number of records ahead to predict.\n",
    "    cols_to_use (list of str): List of column names to use as features.\n",
    "    col_to_pred (str): Column name to predict.\n",
    "    scaler_label (StandardScaler, optional): Scaler for the output variable, used for inverse transformation.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing arrays for training features and labels.\n",
    "    \"\"\"\n",
    "    x_train, y_train = [], []\n",
    "\n",
    "    for df_ in list_dfs:\n",
    "        df = df_.copy()\n",
    "\n",
    "        for i in range(len(df) - lookback - lookforward + 1):\n",
    "            x_train.append(np.asarray(df.iloc[i:i+lookback][cols_to_use].values))\n",
    "            y_train.append(np.asarray(df.iloc[i+lookback][col_to_pred]))\n",
    "\n",
    "    if scaler_label is not None:\n",
    "        y_train = scaler_label.inverse_transform(np.asarray(y_train).reshape(-1,1))\n",
    "\n",
    "    return np.asarray(x_train), np.asarray(y_train)\n",
    "\n",
    "\n",
    "def filter_storms(df, historical_storms, temporal_margin):\n",
    "    \"\"\"\n",
    "    Filter DataFrame entries based on the occurrence of storms within specific time intervals.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing time-series data with a 'Datetime' column.\n",
    "    historical_storms (pandas.DataFrame): DataFrame containing the start and end times of historical storms.\n",
    "    temporal_margin (int): Number of rows before and after the minimum Dst index to include in the result.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of DataFrame snippets corresponding to the specified storm intervals.\n",
    "    \"\"\"\n",
    "    all_storms = []\n",
    "    for i in range(len(historical_storms)):\n",
    "        df_tmp = df[(df[\"Datetime\"] >= historical_storms.iloc[i][\"start\"]) & (df[\"Datetime\"] <= historical_storms.iloc[i][\"end\"])]\n",
    "        idx = df_tmp['Dst'].idxmin()\n",
    "        all_storms.append(df.iloc[idx-temporal_margin:idx+temporal_margin])\n",
    "    return all_storms\n",
    "\n",
    "def combinar_dataframes_solapados(dfs):\n",
    "    \"\"\"\n",
    "    Combines overlapping DataFrames in a list into non-overlapping DataFrames based on the 'Datetime' column.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list of pandas.DataFrame): List of DataFrames to combine.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of combined DataFrames without overlap.\n",
    "    \"\"\"\n",
    "    dfs.sort(key=lambda x: x['Datetime'].min())\n",
    "    combinados = []\n",
    "    combinacion_actual = dfs[0]\n",
    "\n",
    "    for df in dfs[1:]:\n",
    "        if df['Datetime'].min() <= combinacion_actual['Datetime'].max():\n",
    "            combinacion_actual = pd.concat([combinacion_actual, df]).drop_duplicates().sort_values(by='Datetime')\n",
    "        else:\n",
    "            combinados.append(combinacion_actual)\n",
    "            combinacion_actual = df\n",
    "    combinados.append(combinacion_actual)\n",
    "    return combinados\n",
    "\n",
    "def scale_data(list_dfs, cols_to_use, col_to_pred):\n",
    "    \"\"\"\n",
    "    Scales columns in a list of DataFrames using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    list_dfs (list of pandas.DataFrame): List of DataFrames to scale.\n",
    "    cols_to_use (list of str): Column names to apply scaling to.\n",
    "    col_to_pred (str): Column name used as a label for prediction.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the list of scaled DataFrames and the label scaler.\n",
    "    \"\"\"\n",
    "    list_dfs_ = []\n",
    "    scaler_cols = StandardScaler()\n",
    "    scaler_label = StandardScaler()\n",
    "    scaler_cols.fit(pd.concat(list_dfs)[cols_to_use])\n",
    "    scaler_label.fit(np.asarray(pd.concat(list_dfs)[col_to_pred]).reshape(-1,1))\n",
    "\n",
    "    for df_ in list_dfs:\n",
    "        df = df_.copy()\n",
    "        df[cols_to_use] = scaler_cols.transform(df[cols_to_use])\n",
    "        list_dfs_.append(df)\n",
    "\n",
    "    return list_dfs_, scaler_label\n",
    "\n",
    "\n",
    "\n",
    "def calc_metrics(predictions, y_test):\n",
    "  # Mean Squared Error\n",
    "  mse = mean_squared_error(y_test, predictions)\n",
    "  # Mean Absolute Error\n",
    "  mae = mean_absolute_error(y_test, predictions)\n",
    "  # R^2 Score, the coefficient of determination\n",
    "  r2 = r2_score(y_test, predictions)\n",
    "  # Median Absolute Error\n",
    "  medae = median_absolute_error(y_test, predictions)\n",
    "  # Explained Variance Score\n",
    "  explained_variance = explained_variance_score(y_test, predictions)\n",
    "  # Max Error\n",
    "  max_err = max_error(y_test, predictions)\n",
    "\n",
    "  return mse, mae, r2, medae, explained_variance, max_err\n",
    "\n",
    "\n",
    "def formatear_tiempo(segundos):\n",
    "    horas = int(segundos // 3600)\n",
    "    minutos = int((segundos % 3600) // 60)\n",
    "    segundos = segundos % 60\n",
    "    return f\"{horas} horas, {minutos} minutos, {segundos:.2f} segundos\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719831633702,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "dhsyk8pnCibA"
   },
   "outputs": [],
   "source": [
    "def evaluate_svr_models(X_train, y_train, X_test, y_test, kernels, C_values, epsilon_values, gamma_values, degree_values, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate SVR models with different hyperparameter configurations and kernel types.\n",
    "\n",
    "    Args:\n",
    "    X_train (array): Independent training data.\n",
    "    y_train (array): Dependent training data (target).\n",
    "    X_test (array): Independent test data.\n",
    "    y_test (array): Dependent test data (target).\n",
    "    kernels (list): List of kernel types to evaluate.\n",
    "    C_values (list): List of values for the penalty parameter C.\n",
    "    epsilon_values (list): List of values for the epsilon parameter.\n",
    "    gamma_values (list): List of values for the gamma parameter.\n",
    "    degree_values (list): List of values for the degree parameter (used only in polynomial kernels).\n",
    "    verbose (bool, optional): If True, prints messages during the evaluation process. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A pandas DataFrame containing evaluation metrics for each parameter configuration.\n",
    "    dict: A dictionary of trained models, with keys describing the specific parameter configuration.\n",
    "\n",
    "    Note:\n",
    "    Assumes that the 'precomputed' kernel is only used if the 'precomputed_matrix' is defined in the local environment.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    models_dict = {}\n",
    "\n",
    "    for kernel_ in kernels:\n",
    "        for C in C_values:\n",
    "            for epsilon in epsilon_values:\n",
    "                # Check parameter relevance for gamma and degree for the current kernel\n",
    "                relevant_gamma_values = gamma_values if kernel_ in ['rbf', 'sigmoid', 'poly'] else [None]\n",
    "                relevant_degree_values = degree_values if kernel_ == 'poly' else [None]\n",
    "\n",
    "                for gamma in relevant_gamma_values:\n",
    "                    for degree in relevant_degree_values:\n",
    "                        start = time.time()\n",
    "                        config_key = f\"{kernel_}_C{C}_eps{epsilon}_gamma{gamma}_deg{degree}\"\n",
    "                        if verbose:\n",
    "                            print(f\"Starting {config_key}\")\n",
    "\n",
    "                        if kernel_ == \"precomputed\":\n",
    "                            if 'precomputed_matrix' in locals():\n",
    "                                model = SVR(kernel='precomputed', C=C, epsilon=epsilon)\n",
    "                                model.fit(precomputed_matrix, y_train)\n",
    "                                models_dict[config_key] = model\n",
    "                                continue\n",
    "                            else:\n",
    "                                if verbose:\n",
    "                                    print(\"Precomputed matrix not defined for kernel='precomputed'\")\n",
    "                                continue\n",
    "\n",
    "                        model = SVR(kernel=kernel_, C=C, epsilon=epsilon, gamma=(gamma if gamma is not None else 0.01), degree=(degree if degree is not None else 3))\n",
    "                        model.fit(X_train, y_train)\n",
    "                        models_dict[config_key] = model\n",
    "\n",
    "                        predictions = model.predict(X_test)\n",
    "                        mse, mae, r2, medae, explained_variance, max_err = calc_metrics(predictions, y_test)\n",
    "                        end = time.time()\n",
    "                        time_exec = formatear_tiempo(end - start)\n",
    "\n",
    "                        results.append({\n",
    "                            \"kernel\": kernel_,\n",
    "                            \"C\": C,\n",
    "                            \"epsilon\": epsilon,\n",
    "                            \"gamma\": gamma,\n",
    "                            \"degree\": degree,\n",
    "                            \"mse\": mse,\n",
    "                            \"mae\": mae,\n",
    "                            \"r2\": r2,\n",
    "                            \"medae\": medae,\n",
    "                            \"exp_var\": explained_variance,\n",
    "                            \"max_err\": max_err,\n",
    "                            \"time_exec\": time_exec\n",
    "                        })\n",
    "\n",
    "                        print(f\"{config_key} finished. Time for iteration: {time_exec} | mae: {mae} | mse: {mse} | exp_var: {explained_variance} | max_err: {max_err}\")\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results, models_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para extraer las fechas de la nueva división de datos, una vez ampliadas las ventanas de cada tormenta a +-5 días desde el mínimo de cada tormenta, y eliminados los datos solapados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_splits_dates(all_storms):\n",
    "    dates_start = []\n",
    "    dates_end = []\n",
    "    num_storm = []\n",
    "    min_dst_values = []\n",
    "    \n",
    "    for idx, df in enumerate(all_storms):\n",
    "        dates_start.append(df[\"Datetime\"].iloc[0])\n",
    "        dates_end.append(df[\"Datetime\"].iloc[-1])\n",
    "        num_storm.append(idx+1)\n",
    "        min_dst_values.append(df[\"Dst\"].min())\n",
    "    df_new_storms = pd.DataFrame(\n",
    "        {\n",
    "            \"storm_index\": num_storm,\n",
    "            \"date_start\": dates_start,\n",
    "            \"date_end\": dates_end,\n",
    "            \"min_DST\": min_dst_values\n",
    "        }\n",
    "    )\n",
    "    return df_new_storms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para sacar los scalers de las variables de input y de la label (Dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalers(list_dfs, cols_to_use, col_to_pred):\n",
    "    list_dfs_ = []\n",
    "    scaler_cols = StandardScaler()\n",
    "    scaler_label = StandardScaler()\n",
    "    scaler_cols.fit(pd.concat(list_dfs)[cols_to_use])\n",
    "    scaler_label.fit(np.asarray(pd.concat(list_dfs)[col_to_pred]).reshape(-1,1))\n",
    "\n",
    "    return scaler_cols, scaler_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueva función para crear ventanas de datos para input del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window_df_nn(list_dfs, lookback, lookforward, cols_to_use, col_to_pred, scaler_cols):\n",
    "    x_train, y_train = [], []\n",
    "\n",
    "    for df_ in list_dfs:\n",
    "        df = df_[cols_to_use].copy()\n",
    "        df.loc[:, cols_to_use] = scaler_cols.transform(df[cols_to_use])\n",
    "\n",
    "        for i in range(len(df) - lookback - lookforward + 1):\n",
    "            x_train.append(np.asarray(df.iloc[i:i+lookback].values))\n",
    "            y_train.append(np.asarray(df.iloc[i+lookback+lookforward-1][col_to_pred]))\n",
    "\n",
    "    return np.asarray(x_train), np.asarray(y_train)\n",
    "\n",
    "def create_window_df_nn_test(list_dfs, lookback, lookforward, cols_to_use, col_to_pred, scaler_cols):\n",
    "    x_train, y_train, date_pred, date_last_data = [], [], [], []\n",
    "\n",
    "    for df_ in list_dfs:\n",
    "        df = df_.copy()\n",
    "        df.loc[:, cols_to_use] = scaler_cols.transform(df[cols_to_use])\n",
    "\n",
    "        for i in range(len(df) - lookback - lookforward + 1):\n",
    "            x_train.append(np.asarray(df.iloc[i:i+lookback][cols_to_use].values))\n",
    "            y_train.append(np.asarray(df.iloc[i+lookback+lookforward-1][col_to_pred]))\n",
    "            date_last_data.append(df.iloc[i+lookback-1][\"Datetime\"])\n",
    "            date_pred.append(df.iloc[i+lookback+lookforward-1][\"Datetime\"])\n",
    "\n",
    "    return np.asarray(x_train), np.asarray(y_train), np.asarray(date_last_data), np.asarray(date_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, max_error, median_absolute_error\n",
    "from math import sqrt\n",
    "import sklearn\n",
    "\n",
    "def error_nn(y_pred, y_true, index_scaler):\n",
    "    y_pred = index_scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    y_true = index_scaler.inverse_transform(y_true.reshape(-1, 1))\n",
    "    \n",
    "    print(y_pred.reshape(-1).shape)\n",
    "    print(y_true.reshape(-1).shape)\n",
    "    df_ = pd.DataFrame({\"y_pred\": y_pred.reshape(-1), \"y_true\": y_true.reshape(-1)})\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # Median Absolute Error\n",
    "    medae = median_absolute_error(y_true, y_pred)\n",
    "    # Explained Variance Score\n",
    "    explained_variance = explained_variance_score(y_true, y_pred)\n",
    "    # Max Error\n",
    "    max_err = max_error(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "        'Métrica': ['RMSE', 'MSE', 'MAE', 'R²', 'MedAE', 'Varianza explicada', 'Max error'],\n",
    "        'Valor': [rmse, mse, mae, r2, medae, explained_variance, max_err]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df_invertido = df.transpose()\n",
    "    df_invertido.columns = df_invertido.iloc[0]\n",
    "    df_invertido = df_invertido[1:]\n",
    "    display(df_invertido)\n",
    "    \n",
    "    # scatter plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.scatter(y_true, y_pred, c='blue', label='Predicciones vs. Valores reales')\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'k--', lw=2, label='Línea de referencia')\n",
    "    plt.xlabel('Valores reales')\n",
    "    plt.ylabel('Predicciones')\n",
    "    plt.title('Diagrama de dispersión de Predicciones vs. Valores reales')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiZeUiNVpNrd"
   },
   "source": [
    "# 1. Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt5CN8t9pnMA"
   },
   "source": [
    "## 1.1 Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1257,
     "status": "ok",
     "timestamp": 1719831634956,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "o0zK7ahApmQf"
   },
   "outputs": [],
   "source": [
    "hd = pd.read_csv(tfm_path+H1_code+'.csv', parse_dates=[\"Datetime\"])\n",
    "# md = pd.read_csv(tfm_path+M5_code+'.csv', parse_dates=[\"Datetime\"]) # no se va a usar por ahora\n",
    "historical_storms = pd.read_csv(tfm_path+hstorms_data)\n",
    "# historical_storms = historical_storms.drop(columns=['Min. Dst (nT)','Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4mHmDLrtEKs"
   },
   "source": [
    "- Ver qué columnas y tipo de datos contienen los df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "up24FqxwscRS",
    "outputId": "536f508f-4b13-4eb8-b18f-af4d7001a4de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID_IMF', 'ID_plasma', 'Bmag', 'dev_Bmag', 'Bx', 'By_gse', 'Bz_gse',\n",
       "       'By_gsm', 'Bz_gsm', 'dev_Bx', 'dev_By', 'dev_Bz', 'P_density',\n",
       "       'dev_P_density', 'AP', 'dev_AP', 'E_field', 'plasma_T', 'dev_plasma_T',\n",
       "       'plasma_V', 'Dst', 'Datetime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "f3lGenT5tId9",
    "outputId": "0e19c4e4-17da-4544-b3ed-7ffb13dd3881"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID_IMF                  float64\n",
       "ID_plasma               float64\n",
       "Bmag                    float64\n",
       "dev_Bmag                float64\n",
       "Bx                      float64\n",
       "By_gse                  float64\n",
       "Bz_gse                  float64\n",
       "By_gsm                  float64\n",
       "Bz_gsm                  float64\n",
       "dev_Bx                  float64\n",
       "dev_By                  float64\n",
       "dev_Bz                  float64\n",
       "P_density               float64\n",
       "dev_P_density           float64\n",
       "AP                      float64\n",
       "dev_AP                  float64\n",
       "E_field                 float64\n",
       "plasma_T                float64\n",
       "dev_plasma_T            float64\n",
       "plasma_V                float64\n",
       "Dst                     float64\n",
       "Datetime         datetime64[ns]\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "2s-_PYArsgz4",
    "outputId": "69ea9159-795b-4dc6-dc10-fd17097b45ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Min. Dst (nT)', 'start', 'end', 'storm'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_storms.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "oq6T4hJDsm3Q",
    "outputId": "53c5b60c-976a-499b-c55c-560ba4585a11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        int64\n",
       "Min. Dst (nT)     int64\n",
       "start            object\n",
       "end              object\n",
       "storm             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_storms.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyL09t1Ks9jW"
   },
   "source": [
    "- Ordenar las tormentas en caso de que no lo estén\n",
    "- Convertir todas las fechas a datetime de pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719831634957,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "KKh0YUUIq6xc"
   },
   "outputs": [],
   "source": [
    "# Ordenar los dataframes por fecha\n",
    "historical_storms = historical_storms.sort_values(by='start')\n",
    "\n",
    "# convertir las columnas de tiempo a datetime64\n",
    "hd['Datetime']=pd.to_datetime(hd['Datetime'])\n",
    "historical_storms['start']=pd.to_datetime(historical_storms['start'])\n",
    "historical_storms['end']=pd.to_datetime(historical_storms['end'])\n",
    "\n",
    "# Cuando se utilizan los datos a 5 minutos, se unen a 5min\n",
    "#data = pd.merge(md, hd[[\"Datetime\", \"Dst\"]], on='Datetime', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-5Cz91Ap3RH"
   },
   "source": [
    "## 1.3 Limpieza y procesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4RKW9php63d"
   },
   "source": [
    "### 1.3.1 Tratamiento de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1719831635295,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "t7usCL6U8a6U",
    "outputId": "88e0a147-1ef4-402f-815d-fa0902645794"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42472"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(hd.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719831635295,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "FBM6gfRL85Tb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raqga\\AppData\\Local\\Temp\\ipykernel_9960\\1152079126.py:80: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\raqga\\AppData\\Local\\Temp\\ipykernel_9960\\1152079126.py:81: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "hd = imputar_nan(hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719831635295,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "AwfFb2yZ8-dU",
    "outputId": "ff1eb87e-72e3-4471-f1ce-66cf42dfb9de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(hd.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn3SwS3Np93N"
   },
   "source": [
    "### 1.3.2 Normalización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1719831635719,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "tSl0lkmfrHEz"
   },
   "outputs": [],
   "source": [
    "all_storms = filter_storms(hd, historical_storms, temporal_margin)\n",
    "all_storms = combinar_dataframes_solapados(all_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1719831636440,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "z9Fj84tVSCwr"
   },
   "outputs": [],
   "source": [
    "historical_storms_new = get_new_splits_dates(all_storms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storm index de cada conjunto de datos, test y validación, entrenamiento el resto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test_storms = [2, 12, 23, 32, 8, 40, 42, 3, 31]\n",
    "index_val_storms = [25, 13, 1, 37, 33, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TORMENTAS DE ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_index</th>\n",
       "      <th>date_start</th>\n",
       "      <th>date_end</th>\n",
       "      <th>min_DST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2001-03-15 13:00:00</td>\n",
       "      <td>2001-03-25 12:00:00</td>\n",
       "      <td>-149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2001-08-12 21:00:00</td>\n",
       "      <td>2001-08-22 20:00:00</td>\n",
       "      <td>-105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2001-09-26 08:00:00</td>\n",
       "      <td>2001-10-06 07:00:00</td>\n",
       "      <td>-166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2001-10-16 21:00:00</td>\n",
       "      <td>2001-11-02 10:00:00</td>\n",
       "      <td>-187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2002-03-19 09:00:00</td>\n",
       "      <td>2002-03-29 08:00:00</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>2002-05-06 19:00:00</td>\n",
       "      <td>2002-05-16 18:00:00</td>\n",
       "      <td>-110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>2002-05-18 17:00:00</td>\n",
       "      <td>2002-05-28 16:00:00</td>\n",
       "      <td>-109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>2002-07-28 05:00:00</td>\n",
       "      <td>2002-08-07 04:00:00</td>\n",
       "      <td>-102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>2002-09-26 16:00:00</td>\n",
       "      <td>2002-10-06 15:00:00</td>\n",
       "      <td>-176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>2002-11-16 10:00:00</td>\n",
       "      <td>2002-11-26 09:00:00</td>\n",
       "      <td>-128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>2003-05-24 23:00:00</td>\n",
       "      <td>2003-06-03 22:00:00</td>\n",
       "      <td>-144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>2003-06-13 09:00:00</td>\n",
       "      <td>2003-06-23 08:00:00</td>\n",
       "      <td>-141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>2003-07-07 05:00:00</td>\n",
       "      <td>2003-07-17 04:00:00</td>\n",
       "      <td>-105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>2003-08-13 15:00:00</td>\n",
       "      <td>2003-08-23 14:00:00</td>\n",
       "      <td>-148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19</td>\n",
       "      <td>2003-11-15 20:00:00</td>\n",
       "      <td>2003-11-25 19:00:00</td>\n",
       "      <td>-422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>2004-01-17 13:00:00</td>\n",
       "      <td>2004-01-27 12:00:00</td>\n",
       "      <td>-130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21</td>\n",
       "      <td>2004-02-06 17:00:00</td>\n",
       "      <td>2004-02-16 16:00:00</td>\n",
       "      <td>-93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22</td>\n",
       "      <td>2004-03-30 00:00:00</td>\n",
       "      <td>2004-04-08 23:00:00</td>\n",
       "      <td>-117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24</td>\n",
       "      <td>2004-08-25 22:00:00</td>\n",
       "      <td>2004-09-04 21:00:00</td>\n",
       "      <td>-129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>2004-11-07 10:00:00</td>\n",
       "      <td>2004-11-17 09:00:00</td>\n",
       "      <td>-374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26</td>\n",
       "      <td>2005-01-17 05:00:00</td>\n",
       "      <td>2005-01-27 04:00:00</td>\n",
       "      <td>-103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27</td>\n",
       "      <td>2005-05-03 18:00:00</td>\n",
       "      <td>2005-05-13 17:00:00</td>\n",
       "      <td>-110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>28</td>\n",
       "      <td>2005-05-25 13:00:00</td>\n",
       "      <td>2005-06-04 12:00:00</td>\n",
       "      <td>-113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29</td>\n",
       "      <td>2005-06-08 00:00:00</td>\n",
       "      <td>2005-06-17 23:00:00</td>\n",
       "      <td>-106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>2005-08-26 19:00:00</td>\n",
       "      <td>2005-09-05 18:00:00</td>\n",
       "      <td>-122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>33</td>\n",
       "      <td>2011-09-21 23:00:00</td>\n",
       "      <td>2011-10-01 22:00:00</td>\n",
       "      <td>-118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>34</td>\n",
       "      <td>2011-10-20 01:00:00</td>\n",
       "      <td>2011-10-30 00:00:00</td>\n",
       "      <td>-147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>35</td>\n",
       "      <td>2012-03-04 08:00:00</td>\n",
       "      <td>2012-03-14 07:00:00</td>\n",
       "      <td>-145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>36</td>\n",
       "      <td>2012-04-19 04:00:00</td>\n",
       "      <td>2012-04-29 03:00:00</td>\n",
       "      <td>-120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>37</td>\n",
       "      <td>2012-07-10 16:00:00</td>\n",
       "      <td>2012-07-20 15:00:00</td>\n",
       "      <td>-139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>38</td>\n",
       "      <td>2012-09-26 04:00:00</td>\n",
       "      <td>2012-10-14 07:00:00</td>\n",
       "      <td>-122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>39</td>\n",
       "      <td>2012-11-09 07:00:00</td>\n",
       "      <td>2012-11-19 06:00:00</td>\n",
       "      <td>-108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>41</td>\n",
       "      <td>2013-05-27 08:00:00</td>\n",
       "      <td>2013-06-06 07:00:00</td>\n",
       "      <td>-124.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    storm_index          date_start            date_end  min_DST\n",
       "0             1 2001-03-15 13:00:00 2001-03-25 12:00:00   -149.0\n",
       "1             4 2001-08-12 21:00:00 2001-08-22 20:00:00   -105.0\n",
       "2             5 2001-09-26 08:00:00 2001-10-06 07:00:00   -166.0\n",
       "3             6 2001-10-16 21:00:00 2001-11-02 10:00:00   -187.0\n",
       "4             7 2002-03-19 09:00:00 2002-03-29 08:00:00   -100.0\n",
       "5             9 2002-05-06 19:00:00 2002-05-16 18:00:00   -110.0\n",
       "6            10 2002-05-18 17:00:00 2002-05-28 16:00:00   -109.0\n",
       "7            11 2002-07-28 05:00:00 2002-08-07 04:00:00   -102.0\n",
       "8            13 2002-09-26 16:00:00 2002-10-06 15:00:00   -176.0\n",
       "9            14 2002-11-16 10:00:00 2002-11-26 09:00:00   -128.0\n",
       "10           15 2003-05-24 23:00:00 2003-06-03 22:00:00   -144.0\n",
       "11           16 2003-06-13 09:00:00 2003-06-23 08:00:00   -141.0\n",
       "12           17 2003-07-07 05:00:00 2003-07-17 04:00:00   -105.0\n",
       "13           18 2003-08-13 15:00:00 2003-08-23 14:00:00   -148.0\n",
       "14           19 2003-11-15 20:00:00 2003-11-25 19:00:00   -422.0\n",
       "15           20 2004-01-17 13:00:00 2004-01-27 12:00:00   -130.0\n",
       "16           21 2004-02-06 17:00:00 2004-02-16 16:00:00    -93.0\n",
       "17           22 2004-03-30 00:00:00 2004-04-08 23:00:00   -117.0\n",
       "18           24 2004-08-25 22:00:00 2004-09-04 21:00:00   -129.0\n",
       "19           25 2004-11-07 10:00:00 2004-11-17 09:00:00   -374.0\n",
       "20           26 2005-01-17 05:00:00 2005-01-27 04:00:00   -103.0\n",
       "21           27 2005-05-03 18:00:00 2005-05-13 17:00:00   -110.0\n",
       "22           28 2005-05-25 13:00:00 2005-06-04 12:00:00   -113.0\n",
       "23           29 2005-06-08 00:00:00 2005-06-17 23:00:00   -106.0\n",
       "24           30 2005-08-26 19:00:00 2005-09-05 18:00:00   -122.0\n",
       "25           33 2011-09-21 23:00:00 2011-10-01 22:00:00   -118.0\n",
       "26           34 2011-10-20 01:00:00 2011-10-30 00:00:00   -147.0\n",
       "27           35 2012-03-04 08:00:00 2012-03-14 07:00:00   -145.0\n",
       "28           36 2012-04-19 04:00:00 2012-04-29 03:00:00   -120.0\n",
       "29           37 2012-07-10 16:00:00 2012-07-20 15:00:00   -139.0\n",
       "30           38 2012-09-26 04:00:00 2012-10-14 07:00:00   -122.0\n",
       "31           39 2012-11-09 07:00:00 2012-11-19 06:00:00   -108.0\n",
       "32           41 2013-05-27 08:00:00 2013-06-06 07:00:00   -124.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_storms = [df for idx, df in enumerate(all_storms) if idx not in [x-1 for x in index_test_storms]]\n",
    "historical_storms_new[~historical_storms_new[\"storm_index\"].isin(index_test_storms)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TORMENTAS DE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storm_index</th>\n",
       "      <th>date_start</th>\n",
       "      <th>date_end</th>\n",
       "      <th>min_DST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2001-03-26 08:00:00</td>\n",
       "      <td>2001-04-05 07:00:00</td>\n",
       "      <td>-387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2001-04-13 06:00:00</td>\n",
       "      <td>2001-04-27 14:00:00</td>\n",
       "      <td>-114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>2002-04-13 07:00:00</td>\n",
       "      <td>2002-04-25 07:00:00</td>\n",
       "      <td>-149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2002-08-30 05:00:00</td>\n",
       "      <td>2002-09-12 23:00:00</td>\n",
       "      <td>-181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>2004-07-18 02:00:00</td>\n",
       "      <td>2004-08-01 12:00:00</td>\n",
       "      <td>-170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31</td>\n",
       "      <td>2006-04-09 09:00:00</td>\n",
       "      <td>2006-04-19 08:00:00</td>\n",
       "      <td>-98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>2006-12-10 07:00:00</td>\n",
       "      <td>2006-12-20 06:00:00</td>\n",
       "      <td>-162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>2013-03-12 20:00:00</td>\n",
       "      <td>2013-03-22 19:00:00</td>\n",
       "      <td>-132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>2014-02-14 08:00:00</td>\n",
       "      <td>2014-02-24 07:00:00</td>\n",
       "      <td>-119.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   storm_index          date_start            date_end  min_DST\n",
       "0            2 2001-03-26 08:00:00 2001-04-05 07:00:00   -387.0\n",
       "1            3 2001-04-13 06:00:00 2001-04-27 14:00:00   -114.0\n",
       "2            8 2002-04-13 07:00:00 2002-04-25 07:00:00   -149.0\n",
       "3           12 2002-08-30 05:00:00 2002-09-12 23:00:00   -181.0\n",
       "4           23 2004-07-18 02:00:00 2004-08-01 12:00:00   -170.0\n",
       "5           31 2006-04-09 09:00:00 2006-04-19 08:00:00    -98.0\n",
       "6           32 2006-12-10 07:00:00 2006-12-20 06:00:00   -162.0\n",
       "7           40 2013-03-12 20:00:00 2013-03-22 19:00:00   -132.0\n",
       "8           42 2014-02-14 08:00:00 2014-02-24 07:00:00   -119.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_storms = [df for idx, df in enumerate(all_storms) if idx in [x-1 for x in index_test_storms]]\n",
    "historical_storms_new[historical_storms_new[\"storm_index\"].isin(index_test_storms)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos scalers de input y label con todo el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1719831636910,
     "user": {
      "displayName": "Raquel G. Marañón",
      "userId": "10845715467248390754"
     },
     "user_tz": -120
    },
    "id": "dWxN6o_sIFsf"
   },
   "outputs": [],
   "source": [
    "scaler_cols, scaler_label = get_scalers(all_storms, cols_to_use, col_to_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genereamos ventanas en el formato deseado de los conjuntos de entrenamiento test y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape :  (7845, 12, 10) \n",
      "y_train shape: (7845,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = create_window_df_nn(train_storms, lookback, lookforward, cols_to_use, col_to_pred, scaler_cols)\n",
    "print(\"x_train shape : \", x_train.shape, \"\\ny_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape :  (2395, 12, 10) \n",
      "y_test shape: (2395,)\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = create_window_df_nn(test_storms, lookback, lookforward, cols_to_use, col_to_pred, scaler_cols)\n",
    "print(\"x_test shape : \", x_test.shape, \"\\ny_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesario hacer reshape de las ventanas, flatten(). Requerimientos de SVR y algoritmos similares (XGBoost, RandomForest...)  \n",
    "La shape de y_train, y_test se mantiene, comprobamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape :  (7845, 120) \n",
      "y_train shape: (7845,)\n",
      "x_test shape :  (2395, 120) \n",
      "y_test shape: (2395,)\n"
     ]
    }
   ],
   "source": [
    "X_train = x_train.reshape(x_train.shape[0], -1)  # Transforma a (n_samples, n_features*lookback)\n",
    "X_test = x_test.reshape(x_test.shape[0], -1)  # Lo mismo para el conjunto de prueba\n",
    "\n",
    "print(\"x_train shape : \", X_train.shape, \"\\ny_train shape:\", y_train.shape)\n",
    "print(\"x_test shape : \", X_test.shape, \"\\ny_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULHHGwCjqr6C"
   },
   "source": [
    "# 2. Entrenamiento de modelo ÓPTIMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(kernel=\"linear\", C=0.7196856730011522, epsilon=0.1)\n",
    "# Entrenar el modelo SVR con kernel lineal\n",
    "# model = SVR(kernel=\"linear\", C=0.13894954943731375, epsilon=0.1)\n",
    "# model = SVR(kernel=\"linear\", C=0.02682695795279726, epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4200719105.py, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[32], line 50\u001b[1;36m\u001b[0m\n\u001b[1;33m    Explicación:\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Usar RFE para seleccionar 4 características más relevantes\n",
    "rfe = RFE(estimator=model, n_features_to_select=4)\n",
    "rfe.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Mostrar las características seleccionadas\n",
    "selected_features = [feature for feature, selected in zip(X.columns, rfe.support_) if selected]\n",
    "print(f\"Características seleccionadas: {selected_features}\")\n",
    "\n",
    "# Entrenar el modelo con las características seleccionadas\n",
    "X_train_selected = rfe.transform(X_train_scaled)\n",
    "X_test_selected = rfe.transform(X_test_scaled)\n",
    "model.fit(X_train_selected, y_train_scaled)\n",
    "\n",
    "# Predecir y evaluar el modelo\n",
    "y_pred_scaled = model.predict(X_test_selected)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"R²: {r2}\")\n",
    "\n",
    "# Extraer coeficientes y desescalarlos\n",
    "coef = model.coef_.flatten()\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Ajustar los coeficientes a la escala original\n",
    "selected_scaler_X_scale = scaler_X.scale_[rfe.support_]\n",
    "selected_scaler_X_mean = scaler_X.mean_[rfe.support_]\n",
    "\n",
    "coef_descaled = coef / selected_scaler_X_scale\n",
    "intercept_descaled = intercept - (selected_scaler_X_mean @ (coef / selected_scaler_X_scale)) * scaler_y.scale_ + scaler_y.mean_\n",
    "\n",
    "# Mostrar la ecuación desescalada\n",
    "equation = \"VbleTarget = \"\n",
    "for i, (col, c) in enumerate(zip(selected_features, coef_descaled)):\n",
    "    equation += f\"{c:.4f} * {col}(t)\"\n",
    "    if i < len(coef_descaled) - 1:\n",
    "        equation += \" + \"\n",
    "equation += f\" + {intercept_descaled[0]:.4f}\"\n",
    "print(equation)\n",
    "Explicación:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
